Hereâ€™s a text summary of all the insights and steps from your latest.py file for the Demand Forecasting project:

ğŸ“¦ Data Loading & Preparation

Downloaded the M5 Forecasting Accuracy dataset from Kaggle.
Loaded calendar.csv, sell_prices.csv, and sales_train_evaluation.csv.
Used a memory reduction function to optimize large dataframes.
Merged datasets store-wise (e.g., CA_1, TX_1) to avoid memory issues.
Saved merged data as .pkl files for reuse.


ğŸ“Š Exploratory Data Analysis (EDA)
General Insights:

Dataset contains 11M+ rows across two stores.
Sales trends show seasonality and growth over time.
FOODS category and FOODS_3 department dominate in frequency.
Data distribution is balanced between CA_1 and TX_1.

Event Impacts:

Events like Easter, Labor Day â†’ higher average sales.
Events like Christmas, Thanksgiving â†’ lower average sales.
Cultural & Sporting events outperform Religious/National ones.

SNAP Days:

SNAP days positively impact sales in California and Texas.

Price Trends:

Sell prices fluctuate over time for selected items.
Inverse relationship between price and sales (higher price â†’ lower sales).

Temporal Patterns:

Weekends (Saturday, Sunday) show higher sales.
July and August have peak monthly averages.

Categorical Analysis:

Sales vary significantly across stores, categories, departments.
CA_1, FOODS, and FOODS_3 lead in average sales.

Outliers:

Box plots reveal outliers in sales and sell prices across all groups.


ğŸ› ï¸ Feature Engineering

Created lag features: sales_lag7, sales_lag14, sales_lag28.
Created rolling mean features: sales_rollmean7, sales_rollmean28.
Split data into train/validation using d_num (day index).


ğŸ¤– Modeling
Models Used:


LightGBM

Fast, efficient gradient boosting.
RMSE used for evaluation.



XGBoost

Tree-based model with strong performance.
RMSE calculated.



Random Forest

Used for baseline comparison.



LSTM

Captures long-term dependencies in time series.
Trained with 50 epochs and dropout regularization.



GRU

Lightweight alternative to LSTM.
Same training setup as LSTM.




ğŸ“ˆ Evaluation

RMSE used across models for validation.
No early stopping applied.
Feature importance not yet explored.


ğŸ§  Final Insights & Recommendations
Strengths:

Modular and scalable pipeline.
Deep EDA with actionable insights.
Hybrid modeling approach (ML + DL).

Efficient memory and data handling.



ğŸ“¦ Data Loading & Setup

Datasets used: calendar.csv, sell_prices.csv, sales_train_evaluation.csv
Total rows:

calendar.csv: 1,969 rows
sell_prices.csv: 6,841,121 rows
sales_train_evaluation.csv: 30,490 rows, 1,947 columns


Unique values:

Items: 3,049
Stores: 10
Departments: 7
Categories: 3
States: 3




ğŸ§¹ Preprocessing & Memory Optimization

Store-wise merging to avoid memory overload.
Memory reduced by up to 87% using reduce_memory_usage() function.
Final merged dataset per store: 5,918,109 rows, 22 columns


ğŸ“Š Exploratory Data Analysis (EDA)
ğŸ”¹ Sales Trends

Clear seasonality and growth over time.
Weekend sales (Saturday: 1.40, Sunday: 1.45) are higher than weekdays.

ğŸ”¹ Category & Department Distribution

Most frequent category: FOODS (5.57M rows)
Most frequent department: FOODS_3 (3.19M rows)

ğŸ”¹ Event Impact on Sales

Highest average sales:

OrthodoxEaster: 1.47
Easter: 1.45
LaborDay: 1.43


Lowest average sales:

Christmas: 0.00007
Thanksgiving: 0.77
NewYear: 0.88


Event types:

Sporting: 1.19
Cultural: 1.17
Religious: 1.14
National: 1.00



ğŸ”¹ SNAP Days Impact

California:

Non-SNAP: 1.28
SNAP: 1.41


Texas:

Non-SNAP: 0.92
SNAP: 1.04



ğŸ”¹ Price Trends & Elasticity

Average sell price: $4.41
Max price: $107.32
Min price: $0.01
Scatter plot shows inverse relationship between price and sales.

ğŸ”¹ Monthly Sales Averages

Highest: August (1.21), July (1.18)
Lowest: December (1.09)


ğŸ§  Feature Engineering

Lag features: sales_lag7, sales_lag14, sales_lag28
Rolling mean features: sales_rollmean7, sales_rollmean28
Train/Validation split:

Train: 77,205 rows
Validation: 22,795 rows




ğŸ¤– Modeling & Evaluation
ğŸ”¹ LightGBM

RMSE: 2.740
Best iteration: 0

ğŸ”¹ XGBoost

RMSE: 2.809

ğŸ”¹ Random Forest

RMSE: 2.981

ğŸ”¹ LSTM

RMSE: 2.711

ğŸ”¹ GRU

RMSE: 2.711

ğŸ”¹ Bidirectional LSTM

RMSE: 2.797

ğŸ”¹ Tuned LSTM (Best Hyperparameters: 50 units, 0.2 dropout)

RMSE: 2.734


ğŸ“ˆ Model Comparison Summary



ModelRMSELSTM2.711GRU2.711Bidirectional LSTM2.797Tuned LSTM2.734LightGBM2.740XGBoost2.809Random Forest2.981

ğŸ“¤ Forecasting & Submission

Forecasted sales for d_1942 to d_1969 using tuned LSTM.
Created submission file submission.csv with predictions.
Future data shape: 914,700 rows, 19 columns


âœ… Final Insights

LSTM and GRU outperform tree-based models in RMSE.
Event and SNAP effects are significant and should be included in modeling.
Iterative forecasting (using predictions to update lag features) is recommended for better accuracy.
For full M5 submission, include both validation (d_1914â€“d_1941) and evaluation (d_1942â€“d_1969) periods.

https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZjVlZTlhYWYtYjk3Zi00NjczLWExZTctNWYxYzMyNDFkNmY3%40thread.v2/0?context=%7b%22Tid%22%3a%22264b9899-fe1b-430b-9509-2154878d5774%22%2c%22Oid%22%3a%221221bf27-f176-4d9c-92bc-e64cc8ab4a1f%22%7d
