import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
import gc
import os # Import os for listing files

# --- Configuration ---
# Moved inside the function to be store-specific if needed, or keep outside if global
NUM_TOP_ITEMS = 1000  # Increased number of top items
PRICE_ADJUSTMENT_RANGE_FULL = [-0.15, -0.10, -0.05, 0, 0.05, 0.10, 0.15]
PRICE_FLOOR_RATIO = 0.8   # Minimum 80% of base price
BASE_ELASTICITY = -1.5
MIN_ELASTICITY = -0.2       # For low-demand items
SYNTHETIC_PRICE_MULTIPLIER = 1.0  # Use average historical price as base

# --- Define the Pricing Pipeline Function ---
def run_pricing_pipeline_select_file():
    print("\n--- Running Dynamic Pricing Pipeline ---")

    # --- Select Data File ---
    try:
        print("Available merged data files in /content/:")
        # List all .pkl files in the /content/ directory
        available_files = [f for f in os.listdir('/content/') if f.endswith('.pkl') and f.startswith('merged_')]
        if not available_files:
            print("No merged data files found in /content/.")
            return

        for i, file_name in enumerate(available_files):
            print(f"{i+1}: {file_name}")

        while True:
            try:
                choice = int(input(f"Please enter the number of the file you want to use (1-{len(available_files)}): "))
                if 1 <= choice <= len(available_files):
                    selected_file_name = available_files[choice - 1]
                    break
                else:
                    print("Invalid choice. Please enter a number within the range.")
            except ValueError:
                print("Invalid input. Please enter a number.")

        print(f"Selected file: '{selected_file_name}'")

        # Infer store ID from the filename (assuming format merged_STORE_ID.pkl)
        try:
            store_id = selected_file_name.split('_')[1].split('.')[0]
            print(f"Inferred Store ID: {store_id}")
        except IndexError:
             print("Could not infer Store ID from filename. Please ensure the filename is in the format 'merged_STORE_ID.pkl'")
             store_id = "Unknown_Store" # Assign a default or handle as error


        df_store_all = pd.read_pickle(os.path.join('/content/', selected_file_name))
        df_store_all["d_num"] = df_store_all["d"].str[2:].astype(int)
        print("Data loaded successfully.")
    except Exception as e:
        print(f"Error during data file selection or loading: {e}")
        return


    # Load trained GRU model
    try:
        model_gru = load_model("gru_model.keras")
        print("GRU model loaded successfully.")
    except Exception as e:
        print(f"Error loading GRU model: {e}")
        return

    # --- Identify Top Items ---
    try:
        # Load sales_train_evaluation.csv to get total sales for item ranking
        # This assumes sales_train_evaluation.csv is still available in the environment
        sales_eval_path = "/content/sales_train_evaluation.csv"
        if not os.path.exists(sales_eval_path):
             print(f"Error: Required file '{sales_eval_path}' not found. Please ensure it's in the /content directory.")
             return

        sales_eval = pd.read_csv(sales_eval_path)
        sales_eval_store = sales_eval[sales_eval["store_id"] == store_id].copy()

        if sales_eval_store.empty:
             print(f"Warning: No sales data found for store ID '{store_id}' in sales_train_evaluation.csv. Cannot identify top items.")
             # Attempt to proceed with all items if no store-specific sales_eval data is found
             print("Proceeding with all items in the selected file for this store.")
             df_store_top_merged = df_store_all.copy()
             NUM_TOP_ITEMS_ACTUAL = len(df_store_top_merged["id"].unique())
             print(f"Using {NUM_TOP_ITEMS_ACTUAL} items for analysis.")
        else:
            date_cols = [c for c in sales_eval_store.columns if c.startswith("d_")]
            sales_eval_store["total_sales"] = sales_eval_store[date_cols].sum(axis=1)
            top_items = sales_eval_store.sort_values("total_sales", ascending=False).head(NUM_TOP_ITEMS)["id"].tolist()
            df_store_top_merged = df_store_all[df_store_all["id"].isin(top_items)].copy()
            NUM_TOP_ITEMS_ACTUAL = len(top_items)
            print(f"Identified top {NUM_TOP_ITEMS_ACTUAL} items.")

        del sales_eval, sales_eval_store
        gc.collect()
    except Exception as e:
        print(f"Error identifying top items: {e}")
        return


    # --- Feature Engineering ---
    try:
        df_store_top_merged = df_store_top_merged.sort_values(["id", "d_num"]).reset_index(drop=True)
        lags = [7, 14, 28]
        rolling_windows = [7, 28]
        grouped_top = df_store_top_merged.groupby("id", observed=False)

        for lag in lags:
            df_store_top_merged[f"sales_lag{lag}"] = grouped_top["sales"].shift(lag)
        for w in rolling_windows:
            col_name = f"sales_rollmean{w}"
            # Ensure the column exists before calculating rolling mean
            if 'sales' in df_store_top_merged.columns:
                df_store_top_merged[col_name] = (
                    grouped_top["sales"]
                    .shift(1)                      # shift by 1 to avoid including current day
                    .rolling(w, min_periods=1)
                    .mean()
                )
            else:
                df_store_top_merged[col_name] = 0.0 # Or handle as appropriate if 'sales' is missing


        feature_cols = [f"sales_lag{x}" for x in lags] + [f"sales_rollmean{x}" for x in rolling_windows]
        # Ensure all feature_cols exist before filling NaNs
        existing_feature_cols = [col for col in feature_cols if col in df_store_top_merged.columns]
        df_store_top_merged[existing_feature_cols] = df_store_top_merged[existing_feature_cols].fillna(0)

        print("Feature engineering complete.")
    except Exception as e:
        print(f"Error during feature engineering: {e}")
        return

    # --- Predict Demand using GRU ---
    try:
        # Ensure feature_cols are present in the DataFrame before selecting
        missing_features = [col for col in feature_cols if col not in df_store_top_merged.columns]
        if missing_features:
             print(f"Error: Missing feature columns after engineering: {missing_features}")
             return

        X_predict = df_store_top_merged[feature_cols].values.reshape((df_store_top_merged.shape[0], 1, len(feature_cols)))
        predicted_demand = model_gru.predict(X_predict)
        df_store_top_merged["predicted_demand_raw"] = predicted_demand.flatten()
        print("Demand prediction complete.")

        # --- Scale Predicted Demand to Historical Average ---
        item_avg_sales = df_store_top_merged.groupby("id")["sales"].mean()
        item_predicted_avg = df_store_top_merged.groupby("id")["predicted_demand_raw"].mean()

        # Scaling factor = historical / predicted
        # Handle cases where item_predicted_avg might be zero or item_avg_sales is zero
        scaling_factor = (item_avg_sales / item_predicted_avg)
        scaling_factor = scaling_factor.replace([np.inf, -np.inf], np.nan).fillna(1.0) # Replace inf with NaN and fill NaN with 1.0


        # Apply scaling
        df_store_top_merged["predicted_demand"] = df_store_top_merged.apply(
            lambda row: max(row["predicted_demand_raw"] * scaling_factor.get(row["id"], 1.0), 0), axis=1
        )
        print("Predicted demand scaled.")
    except Exception as e:
        print(f"Error during demand prediction or scaling: {e}")
        return


    # --- Handle Missing/Zero Sell Prices ---
    try:
        # Calculate item_avg_prices only for the items in df_store_top_merged
        item_avg_prices = df_store_top_merged.groupby("id")["sell_price"].mean()

        def fill_synthetic_price(row):
            if pd.isna(row["sell_price"]) or row["sell_price"] <= 0:
                # Use the average price calculated only for the top items
                base_price = item_avg_prices.get(row["id"], 1.0) # Default to 1.0 if item not in calculated averages
                return base_price * SYNTHETIC_PRICE_MULTIPLIER
            return row["sell_price"]
        df_store_top_merged["sell_price"] = df_store_top_merged.apply(fill_synthetic_price, axis=1)
        print("Missing/zero sell prices handled.")
    except Exception as e:
        print(f"Error handling sell prices: {e}")
        return

    # --- Dynamic Pricing Optimization ---
    try:
        def calculate_adjusted_demand(current_price, predicted_demand, candidate_price, elasticity):
            # Ensure current_price is not zero to avoid division by zero
            if current_price <= 0:
                 return predicted_demand  # Or handle as appropriate for your case
            # Apply elasticity formula
            adjusted_demand = predicted_demand * ((candidate_price / current_price) ** elasticity)
            return adjusted_demand

        def calculate_expected_revenue(adjusted_demand, candidate_price):
            return max(adjusted_demand * candidate_price, 0) # Ensure non-negative revenue


        df_optimal_prices = []

        for index, row in df_store_top_merged.iterrows():
            current_price = row["sell_price"]
            predicted_demand = row["predicted_demand"]

            # Low-demand items get restricted price adjustments and lower elasticity
            # Using a more robust check for low predicted demand
            if predicted_demand < 0.5: # Threshold can be adjusted
                PRICE_ADJUSTMENT_RANGE = [0, 0.05, 0.10] # Offer discounts or no change
                elasticity = MIN_ELASTICITY
            else:
                PRICE_ADJUSTMENT_RANGE = PRICE_ADJUSTMENT_RANGE_FULL # Full range of adjustments
                elasticity = BASE_ELASTICITY

            best_revenue = -np.inf
            optimal_price = current_price

            for adjustment in PRICE_ADJUSTMENT_RANGE:
                candidate_price = current_price * (1 + adjustment)
                candidate_price = max(candidate_price, current_price * PRICE_FLOOR_RATIO) # Apply price floor

                adjusted_demand = calculate_adjusted_demand(current_price, predicted_demand, candidate_price, elasticity)
                adjusted_demand = max(adjusted_demand, 0) # Ensure non-negative demand
                expected_revenue = calculate_expected_revenue(adjusted_demand, candidate_price)

                if expected_revenue > best_revenue:
                    best_revenue = expected_revenue
                    optimal_price = candidate_price

            df_optimal_prices.append({
                "id": row["id"],
                "date": row["date"],
                "current_price": current_price,
                "predicted_demand": predicted_demand,
                "recommended_price": optimal_price,
                "expected_revenue": best_revenue
            })

        df_optimal_prices = pd.DataFrame(df_optimal_prices)
        print("Dynamic pricing optimization complete.")
    except Exception as e:
        print(f"Error during dynamic pricing optimization: {e}")
        return

    # --- Revenue Comparison ---
    try:
        # Ensure the merge keys exist in both dataframes
        merge_keys = ["id", "date", "predicted_demand"]
        if not all(key in df_store_top_merged.columns for key in merge_keys):
             print(f"Error: Missing merge keys in df_store_top_merged: {list(set(merge_keys) - set(df_store_top_merged.columns))}")
             return
        if not all(key in df_optimal_prices.columns for key in merge_keys):
             print(f"Error: Missing merge keys in df_optimal_prices: {list(set(merge_keys) - set(df_optimal_prices.columns))}")
             return


        df_eval_elasticity = pd.merge(
            df_store_top_merged,
            df_optimal_prices,
            on=merge_keys, # Merge on multiple keys
            how="left"
        )
        df_eval_elasticity["actual_revenue_historical"] = df_eval_elasticity["sales"] * df_eval_elasticity["sell_price"]

        total_actual_revenue_historical = df_eval_elasticity["actual_revenue_historical"].sum()
        total_estimated_revenue_elasticity = df_eval_elasticity["expected_revenue"].sum()
        revenue_change_elasticity = total_estimated_revenue_elasticity - total_actual_revenue_historical
        revenue_change_percentage_elasticity = (revenue_change_elasticity / total_actual_revenue_historical) * 100 if total_actual_revenue_historical > 0 else 0

        print(f"\n--- Revenue Impact for Store: {store_id} ---")
        print(f"Total Actual Revenue: ${total_actual_revenue_historical:,.2f}")
        print(f"Total Estimated Revenue: ${total_estimated_revenue_elasticity:,.2f}")
        print(f"Revenue Change: ${revenue_change_elasticity:,.2f}")
        print(f"Revenue Change %: {revenue_change_percentage_elasticity:.2f}%")

        # --- Visualization ---
        revenue_comparison_data = pd.DataFrame({
            'Revenue Type': ['Actual Historical Revenue', 'Estimated Revenue (Dynamic Pricing)'],
            'Total Revenue': [total_actual_revenue_historical, total_estimated_revenue_elasticity]
        })
        plt.figure(figsize=(8, 5))
        sns.barplot(x='Revenue Type', y='Total Revenue', data=revenue_comparison_data)
        plt.title(f'Revenue Comparison (Top {NUM_TOP_ITEMS_ACTUAL} Items, {store_id})')
        plt.ylabel('Total Revenue ($)')
        plt.show()
        print("Revenue comparison plot generated.")
    except Exception as e:
        print(f"Error during revenue comparison or visualization: {e}")
        return

    # --- Cleanup ---
    del df_store_all, df_store_top_merged, df_optimal_prices, df_eval_elasticity
    gc.collect()
    print(f"Cleanup complete for store: {store_id}")

# --- Run the Pipeline ---
# run_pricing_pipeline_select_file() # Keep the definition, but run it in a separate cell






# --- Run the Pipeline ---
run_pricing_pipeline_select_file() # Keep the definition, but run it in a separate cell



