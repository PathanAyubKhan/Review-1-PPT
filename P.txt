    
"""
M5 Forecasting & Dynamic Pricing Dashboard
===========================================
A Streamlit application for demand forecasting and dynamic pricing optimization
using GRU deep learning model.
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from pathlib import Path
import gc
import os
import warnings
import subprocess
import sys
warnings.filterwarnings("ignore")

# ====================
# Install & import gdown
# ====================
try:
    import gdown
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown"])
    import gdown

# TensorFlow imports
try:
    from tensorflow.keras.models import load_model
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    st.error("TensorFlow is not installed. Please install it to use this app.")

# ====================
# Configuration
# ====================
DATA_DIR = Path("data/merged")
MODEL_DIR = Path("models")
PROCESSED_DIR = Path("processed")

# Google Drive file link (Replace with your file if needed)
GDRIVE_LINK = "https://drive.google.com/file/d/1hT2XUq-UV7QKqvqIBzy8N86jl_Ihi5Wr/view?usp=drivesdk"

# Pricing parameters
DEFAULT_ELASTICITY = -1.5
DEFAULT_MIN_ELASTICITY = -0.2
DEFAULT_PRICE_FLOOR = 0.8
PRICE_ADJUSTMENTS_FULL = [-0.15, -0.10, -0.05, 0, 0.05, 0.10, 0.15]
PRICE_ADJUSTMENTS_LIMITED = [0, 0.05, 0.10]

# ====================
# Helper: Download from Google Drive
# ====================
@st.cache_data
def download_data_from_gdrive(gdrive_url, output_path="merged_data.pkl"):
    """Download pickle file from Google Drive and return local path."""
    file_id = gdrive_url.split("/d/")[1].split("/")[0]
    direct_url = f"https://drive.google.com/uc?id={file_id}"
    try:
        gdown.download(direct_url, output_path, quiet=False)
        return output_path
    except Exception as e:
        st.error(f"‚ùå Failed to download data from Google Drive: {e}")
        return None

# ====================
# Caching Functions
# ====================
@st.cache_data
def load_merged_data(file_path):
    """Load merged pickle data with caching."""
    try:
        df = pd.read_pickle(file_path)
        return df
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return None

@st.cache_resource
def load_gru_model(model_path):
    """Load GRU model with caching."""
    if not TENSORFLOW_AVAILABLE:
        return None
    try:
        model = load_model(str(model_path))
        return model
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None

# ====================
# Feature Engineering
# ====================
def create_lag_features(df, lags=[7, 14, 28], rolling_windows=[7, 28]):
    st.info("Creating lag and rolling features...")
    if "d_num" not in df.columns:
        df["d_num"] = df["d"].str[2:].astype(int)
    df = df.sort_values(["id", "d_num"]).reset_index(drop=True)
    grouped = df.groupby("id", observed=False)
    for lag in lags:
        df[f"sales_lag{lag}"] = grouped["sales"].shift(lag)
    for w in rolling_windows:
        df[f"sales_rollmean{w}"] = grouped["sales"].shift(1).rolling(w, min_periods=1).mean()
    feature_cols = [f"sales_lag{x}" for x in lags] + [f"sales_rollmean{x}" for x in rolling_windows]
    df[feature_cols] = df[feature_cols].fillna(0)
    return df, feature_cols

# ====================
# Demand Forecasting
# ====================
def predict_demand(df, model, feature_cols):
    st.info("Predicting demand with GRU model...")
    X_predict = df[feature_cols].values.reshape((df.shape[0], 1, len(feature_cols)))
    with st.spinner("Running GRU predictions..."):
        predicted_demand = model.predict(X_predict, verbose=0)
    df["predicted_demand_raw"] = predicted_demand.flatten()
    item_avg_sales = df.groupby("id")["sales"].mean()
    item_predicted_avg = df.groupby("id")["predicted_demand_raw"].mean()
    scaling_factor = (item_avg_sales / item_predicted_avg).replace([np.inf, -np.inf], np.nan).fillna(1.0)
    df["predicted_demand"] = df.apply(
        lambda row: max(row["predicted_demand_raw"] * scaling_factor.get(row["id"], 1.0), 0),
        axis=1
    )
    return df

# ====================
# Price Handling
# ====================
def handle_missing_prices(df):
    st.info("Handling missing prices...")
    item_avg_prices = df.groupby("id")["sell_price"].mean()
    def fill_synthetic_price(row):
        if pd.isna(row["sell_price"]) or row["sell_price"] <= 0:
            base_price = item_avg_prices.get(row["id"], 1.0)
            return base_price
        return row["sell_price"]
    df["sell_price"] = df.apply(fill_synthetic_price, axis=1)
    return df

# ====================
# Dynamic Pricing
# ====================
def calculate_adjusted_demand(current_price, predicted_demand, candidate_price, elasticity):
    if current_price <= 0:
        return predicted_demand
    return predicted_demand * ((candidate_price / current_price) ** elasticity)

def optimize_pricing(df, base_elasticity, min_elasticity, price_floor_ratio):
    st.info("Optimizing prices...")
    optimal_prices = []
    progress_bar = st.progress(0)
    total_rows = len(df)
    for idx, (index, row) in enumerate(df.iterrows()):
        current_price = row["sell_price"]
        predicted_demand = row["predicted_demand"]
        if predicted_demand < 0.5:
            price_adjustments = PRICE_ADJUSTMENTS_LIMITED
            elasticity = min_elasticity
        else:
            price_adjustments = PRICE_ADJUSTMENTS_FULL
            elasticity = base_elasticity
        best_revenue = -np.inf
        optimal_price = current_price
        for adjustment in price_adjustments:
            candidate_price = current_price * (1 + adjustment)
            candidate_price = max(candidate_price, current_price * price_floor_ratio)
            adjusted_demand = calculate_adjusted_demand(
                current_price, predicted_demand, candidate_price, elasticity
            )
            adjusted_demand = max(adjusted_demand, 0)
            expected_revenue = adjusted_demand * candidate_price
            if expected_revenue > best_revenue:
                best_revenue = expected_revenue
                optimal_price = candidate_price
        optimal_prices.append({
            "id": row["id"],
            "item_id": row["item_id"],
            "date": row["date"],
            "current_price": current_price,
            "predicted_demand": predicted_demand,
            "optimized_price": optimal_price,
            "optimized_revenue": best_revenue
        })
        if idx % 1000 == 0:
            progress_bar.progress(min(idx / total_rows, 1.0))
    progress_bar.progress(1.0)
    return pd.DataFrame(optimal_prices)

# ====================
# Revenue Analysis
# ====================
def calculate_revenue_impact(df, df_optimal):
    df_eval = pd.merge(
        df,
        df_optimal,
        on=["id", "date", "predicted_demand"],
        how="left"
    )
    df_eval["actual_revenue"] = df_eval["sales"] * df_eval["sell_price"]
    total_actual = df_eval["actual_revenue"].sum()
    total_estimated = df_eval["optimized_revenue"].sum()
    revenue_change = total_estimated - total_actual
    revenue_change_pct = (revenue_change / total_actual * 100) if total_actual > 0 else 0
    return {
        "total_actual": total_actual,
        "total_estimated": total_estimated,
        "revenue_change": revenue_change,
        "revenue_change_pct": revenue_change_pct,
        "df_eval": df_eval
    }

# ====================
# Visualization
# ====================
def plot_revenue_comparison(actual, estimated, store_id):
    fig = go.Figure(data=[
        go.Bar(
            x=["Actual Historical Revenue", "Estimated Revenue (Dynamic Pricing)"],
            y=[actual, estimated],
            marker_color=["#1f77b4", "#ff7f0e"],
            text=[f"${actual:,.0f}", f"${estimated:,.0f}"],
            textposition="auto",
        )
    ])
    fig.update_layout(
        title=f"Revenue Comparison - {store_id}",
        yaxis_title="Total Revenue ($)",
        height=500,
        showlegend=False
    )
    return fig

def plot_price_distribution(df_optimal):
    df_optimal["price_change_pct"] = (
        (df_optimal["optimized_price"] - df_optimal["current_price"]) / df_optimal["current_price"] * 100
    )
    fig = px.histogram(
        df_optimal,
        x="price_change_pct",
        nbins=50,
        title="Distribution of Price Changes",
        labels={"price_change_pct": "Price Change (%)"}
    )
    fig.update_layout(height=400)
    return fig

# ====================
# Main App
# ====================
def main():
    st.set_page_config(
        page_title="M5 Forecasting & Dynamic Pricing",
        page_icon="üìä",
        layout="wide"
    )
    st.title("üìä M5 Forecasting & Dynamic Pricing Dashboard")
    st.markdown("---")

    st.sidebar.header("‚öôÔ∏è Configuration")

    merged_files = []
    possible_dirs = [DATA_DIR, PROCESSED_DIR, Path(".")]
    for directory in possible_dirs:
        if directory.exists():
            merged_files.extend(list(directory.glob("merged_*.pkl")))

    # If no local file found, download from Google Drive
    if not merged_files:
        st.warning("‚ö†Ô∏è No merged data files found locally. Attempting to download from Google Drive...")
        downloaded_file = download_data_from_gdrive(GDRIVE_LINK)
        if downloaded_file:
            merged_files.append(Path(downloaded_file))
            st.success("‚úÖ Data file downloaded successfully!")
        else:
            st.error("‚ùå Could not load or download the data file.")
            return

    file_names = [f.name for f in merged_files]
    selected_file_name = st.sidebar.selectbox("üìÅ Select Store Data", file_names)
    try:
        store_id = selected_file_name.split("_")[1].split(".")[0]
    except:
        store_id = "Unknown"

    st.sidebar.info(f"**Store ID:** {store_id}")

    elasticity = st.sidebar.slider(
        "Base Price Elasticity", min_value=-3.0, max_value=-0.1,
        value=DEFAULT_ELASTICITY, step=0.1
    )
    min_elasticity = st.sidebar.slider(
        "Min Elasticity (Low Demand)", min_value=-1.0, max_value=-0.1,
        value=DEFAULT_MIN_ELASTICITY, step=0.1
    )
    price_floor = st.sidebar.slider(
        "Price Floor Ratio", min_value=0.5, max_value=1.0,
        value=DEFAULT_PRICE_FLOOR, step=0.05
    )

    model_files = list(MODEL_DIR.glob("*.h5")) + list(MODEL_DIR.glob("*.keras"))
    model_files_root = list(Path(".").glob("*.h5")) + list(Path(".").glob("*.keras"))
    model_files.extend(model_files_root)
    if not model_files:
        st.sidebar.error("‚ùå No model files found!")
        st.error("""
        **Model file not found!**
        Please ensure the GRU model file exists in:
        - models/ or root directory
        """)
        return
    model_file = model_files[0]
    st.sidebar.success(f"‚úÖ Model: {model_file.name}")

    run_pipeline = st.sidebar.button("üöÄ Run Pipeline", type="primary")

    if run_pipeline:
        st.header(f"Running Pipeline for Store: **{store_id}**")
        selected_file_path = None
        for f in merged_files:
            if f.name == selected_file_name:
                selected_file_path = f
                break
        if selected_file_path is None:
            st.error("Could not find selected file!")
            return

        with st.spinner("Loading data..."):
            df = load_merged_data(selected_file_path)
        if df is None:
            return
        st.success(f"‚úÖ Data loaded: {df.shape[0]:,} rows, {df.shape[1]} columns")

        with st.spinner("Loading GRU model..."):
            model = load_gru_model(model_file)
        if model is None:
            return
        st.success("‚úÖ Model loaded successfully")

        df, feature_cols = create_lag_features(df)
        st.success(f"‚úÖ Features created: {len(feature_cols)} features")

        df = predict_demand(df, model, feature_cols)
        st.success("‚úÖ Demand predictions complete")

        df = handle_missing_prices(df)
        st.success("‚úÖ Prices processed")

        df_optimal = optimize_pricing(df, elasticity, min_elasticity, price_floor)
        st.success("‚úÖ Price optimization complete")

        results = calculate_revenue_impact(df, df_optimal)

        st.markdown("---")
        st.header("üìà Results")

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Actual Revenue", f"${results['total_actual']:,.0f}")
        with col2:
            st.metric("Estimated Revenue", f"${results['total_estimated']:,.0f}")
        with col3:
            st.metric("Revenue Change", f"${results['revenue_change']:,.0f}", delta=f"{results['revenue_change_pct']:.2f}%")
        with col4:
            st.metric("Change %", f"{results['revenue_change_pct']:.2f}%")

        st.subheader("Revenue Comparison")
        fig = plot_revenue_comparison(results['total_actual'], results['total_estimated'], store_id)
        st.plotly_chart(fig, use_container_width=True)

        col1, col2 = st.columns(2)
        with col1:
            st.subheader("Price Change Distribution")
            fig = plot_price_distribution(df_optimal)
            st.plotly_chart(fig, use_container_width=True)
        with col2:
            st.subheader("Summary Statistics")
            avg_price_change = ((df_optimal["optimized_price"] - df_optimal["current_price"]) / df_optimal["current_price"] * 100).mean()
            st.write(f"**Average Price Change:** {avg_price_change:.2f}%")
            st.write(f"**Total Items:** {df_optimal['item_id'].nunique():,}")
            st.write(f"**Total Observations:** {len(df_optimal):,}")

        st.subheader("üèÜ Top 10 Items by Optimized Revenue")
        item_summary = df_optimal.groupby("item_id").agg({
            "current_price": "mean",
            "optimized_price": "mean",
            "predicted_demand": "sum",
            "optimized_revenue": "sum"
        }).reset_index()
        top_items = item_summary.nlargest(10, "optimized_revenue")
        display_df = top_items.copy()
        display_df["current_price"] = display_df["current_price"].apply(lambda x: f"${x:.2f}")
        display_df["optimized_price"] = display_df["optimized_price"].apply(lambda x: f"${x:.2f}")
        display_df["predicted_demand"] = display_df["predicted_demand"].apply(lambda x: f"{x:.0f}")
        display_df["optimized_revenue"] = display_df["optimized_revenue"].apply(lambda x: f"${x:,.2f}")
        st.dataframe(display_df, use_container_width=True)

        csv = top_items.to_csv(index=False)
        st.download_button(
            label="üì• Download Top Items CSV",
            data=csv,
            file_name=f"top_items_{store_id}.csv",
            mime="text/csv"
        )

        st.subheader("üìä Full Results")
        full_csv = df_optimal.to_csv(index=False)
        st.download_button(
            label="üì• Download Full Results CSV",
            data=full_csv,
            file_name=f"full_results_{store_id}.csv",
            mime="text/csv"
        )

        del df, df_optimal, results
        gc.collect()
    else:
        st.info("""
        ### üìã Instructions
        1. Select a store from the sidebar dropdown.
        2. Adjust pricing parameters (optional).
        3. Click "Run Pipeline".
        The app will:
        - Load data (from GDrive if needed)
        - Create time-series features
        - Predict demand with GRU
        - Optimize prices
        - Show revenue gains.
        """)

        st.subheader("üìÅ Available Data Files")
        st.write([f.name for f in merged_files])

if __name__ == "__main__":
    main()

https://drive.google.com/file/d/13eKDm3pO2-befMKokPS6CHY0I6P51Esd/view?usp=sharing



https://drive.google.com/file/d/1hT2XUq-UV7QKqvqIBzy8N86jl_Ihi5Wr/view?usp=drivesdk



