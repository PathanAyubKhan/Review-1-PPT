{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f4aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# üì¶ Essential Python Imports\n",
    "# ============================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================\n",
    "# üß™ Data & Scientific Libraries\n",
    "# ============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================\n",
    "# üìä Machine Learning & Forecasting\n",
    "# ============================\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# ============================\n",
    "# üìà Visualization\n",
    "# ============================\n",
    "import plotly.express as px\n",
    "\n",
    "# ============================\n",
    "# üß† Deep Learning\n",
    "# ============================\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# ============================\n",
    "# ‚úÖ Confirmation\n",
    "# ============================\n",
    "print(\"‚úÖ All libraries loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed07b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the Calendar Dataset\n",
    "calendar_path = \"calendar.csv\"\n",
    "calendar = pd.read_csv(calendar_path)\n",
    "\n",
    "# Print dataset shape\n",
    "print(\"Calendar shape:\", calendar.shape)\n",
    "\n",
    "# Display the first 5 rows\n",
    "calendar.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Checking for missing values, data types & descriptive stats\n",
    "\n",
    "print(\"-- Missing values in calendar --\")\n",
    "print(calendar.isnull().sum())\n",
    "\n",
    "print(\"\\n-- Data types (info) --\")\n",
    "calendar.info()\n",
    "\n",
    "print(\"\\n-- Descriptive statistics --\")\n",
    "print(calendar.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3619c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading sell_prices.csv\n",
    "sell_prices_path = \"sell_prices.csv\"\n",
    "sell_prices = pd.read_csv(sell_prices_path)\n",
    "\n",
    "# Print dataset shape\n",
    "print(\"Sell Prices shape:\", sell_prices.shape)\n",
    "\n",
    "# Display first 5 rows\n",
    "sell_prices.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Checking for missing values, data types & descriptive stats in sell_prices\n",
    "\n",
    "print(\"-- Missing values in sell_prices --\")\n",
    "print(sell_prices.isnull().sum())\n",
    "\n",
    "print(\"\\n-- Data types (info) --\")\n",
    "sell_prices.info()\n",
    "\n",
    "print(\"\\n-- Descriptive stats for 'sell_price' --\")\n",
    "print(sell_prices[\"sell_price\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the sales_train_evaluation.csv\n",
    "sales_eval_path = \"sales_train_evaluation.csv\"\n",
    "sales_eval = pd.read_csv(sales_eval_path)\n",
    "\n",
    "# Check shape\n",
    "print(\"Sales Evaluation shape:\", sales_eval.shape)\n",
    "\n",
    "# Preview first 5 rows\n",
    "sales_eval.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99981542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Checking missing values and data types in sales_eval\n",
    "\n",
    "print(\"-- Missing values in sales_eval --\")\n",
    "total_missing = sales_eval.isnull().sum().sum()\n",
    "print(f\"Total missing values: {total_missing}\")\n",
    "\n",
    "print(\"\\n-- Data types (info) --\")\n",
    "sales_eval.info(verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ce459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Check unique values before any reshaping or merging\n",
    "\n",
    "print(\"Unique item IDs:\", sales_eval[\"item_id\"].nunique())\n",
    "print(\"Unique store IDs:\", sales_eval[\"store_id\"].nunique())\n",
    "print(\"Unique dept IDs:\", sales_eval[\"dept_id\"].nunique())\n",
    "print(\"Unique category IDs:\", sales_eval[\"cat_id\"].nunique())\n",
    "print(\"Unique state IDs:\", sales_eval[\"state_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f5b8b",
   "metadata": {},
   "source": [
    "### **Store-by-Store Merge**\n",
    "To avoid running out of memory, we'll process each store_id separately:\n",
    "\n",
    "1. Filter sales_eval for one store.\n",
    "2. Melt the subset so d_1 ... d_1941 become a \"day\" column and a \"sales\" column.\n",
    "3. Merge with calendar on \"d\".\n",
    "4. Filter sell_prices for that store and merge on [store_id, item_id, wm_yr_wk].\n",
    "5. Save the result as a separate .pkl file.\n",
    "6. Clear variables and move on to the next store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141379a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================\n",
    "# Memory Reduction Function\n",
    "# =============================\n",
    "def reduce_memory_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"  Memory usage: {start_mem:.2f} MB ->\", end=\" \")\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type not in [object, 'category']:\n",
    "            df[col] = df[col].fillna(0)\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type).startswith('int'):\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "        else:\n",
    "            if col_type == object and df[col].nunique() / len(df[col]) < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"{end_mem:.2f} MB (reduced by {(start_mem - end_mem)/start_mem*100:.1f}%)\")\n",
    "    return df\n",
    "\n",
    "# =============================\n",
    "# File Paths (Local)\n",
    "# =============================\n",
    "DATA_DIR = \"data\"\n",
    "OUTPUT_DIR = \"processed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "sales_eval_path = os.path.join(\"sales_train_evaluation.csv\")\n",
    "calendar_path = os.path.join(\"calendar.csv\")\n",
    "sell_prices_path = os.path.join(\"sell_prices.csv\")\n",
    "\n",
    "# =============================\n",
    "# Load Datasets\n",
    "# =============================\n",
    "sales_eval = pd.read_csv(sales_eval_path)\n",
    "calendar = pd.read_csv(calendar_path)\n",
    "sell_prices = pd.read_csv(sell_prices_path)\n",
    "\n",
    "print(\"Datasets loaded successfully!\")\n",
    "\n",
    "# =============================\n",
    "# Main Processing Loop\n",
    "# =============================\n",
    "all_stores = sales_eval[\"store_id\"].unique()\n",
    "print(\"Found store_ids:\", all_stores)\n",
    "\n",
    "for store in all_stores:\n",
    "    print(f\"\\nProcessing store: {store}\")\n",
    "\n",
    "    # (1) Filter for this store\n",
    "    df_store = sales_eval[sales_eval[\"store_id\"] == store].copy()\n",
    "    df_store = reduce_memory_usage(df_store)\n",
    "\n",
    "    # (2) Melt the subset\n",
    "    fixed_cols = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "    date_cols = [c for c in df_store.columns if c.startswith(\"d_\")]\n",
    "\n",
    "    df_melted_sub = pd.melt(\n",
    "        df_store,\n",
    "        id_vars=fixed_cols,\n",
    "        value_vars=date_cols,\n",
    "        var_name=\"d\",\n",
    "        value_name=\"sales\"\n",
    "    )\n",
    "    del df_store\n",
    "    gc.collect()\n",
    "\n",
    "    # (3) Merge with calendar\n",
    "    df_cal_sub = pd.merge(df_melted_sub, calendar, how=\"left\", on=\"d\")\n",
    "    del df_melted_sub\n",
    "    gc.collect()\n",
    "\n",
    "    # (4) Filter sell_prices for this store\n",
    "    sp_sub = sell_prices[sell_prices[\"store_id\"] == store].copy()\n",
    "    sp_sub = reduce_memory_usage(sp_sub)\n",
    "\n",
    "    df_merged_sub = pd.merge(\n",
    "        df_cal_sub,\n",
    "        sp_sub,\n",
    "        how=\"left\",\n",
    "        on=[\"store_id\", \"item_id\", \"wm_yr_wk\"]\n",
    "    )\n",
    "    del df_cal_sub, sp_sub\n",
    "    gc.collect()\n",
    "\n",
    "    df_merged_sub = reduce_memory_usage(df_merged_sub)\n",
    "\n",
    "    # (5) Save to disk (local path)\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"merged_{store}.pkl\")\n",
    "    df_merged_sub.to_pickle(out_path)\n",
    "    print(f\"  Saved merged data for store={store}, shape={df_merged_sub.shape} -> {out_path}\")\n",
    "\n",
    "    # (6) Clear memory\n",
    "    del df_merged_sub\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975c62b",
   "metadata": {},
   "source": [
    "## Load merged data\n",
    "\n",
    "### Subtask:\n",
    "Load the merged data for one or more stores to perform the EDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90898a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Folder containing all merged pickle files\n",
    "PROCESSED_DIR = \"processed\"\n",
    "\n",
    "# Automatically get all pickle files in the folder\n",
    "merged_files = glob.glob(os.path.join(PROCESSED_DIR, \"merged_*.pkl\"))\n",
    "\n",
    "# Initialize an empty list to store the loaded DataFrames\n",
    "list_of_dfs = []\n",
    "\n",
    "# Iterate and load each pickle file\n",
    "for file_path in merged_files:\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    df_sub = pd.read_pickle(file_path)\n",
    "    print(f\"  Shape of loaded data: {df_sub.shape}\")\n",
    "    list_of_dfs.append(df_sub)\n",
    "    # Free memory\n",
    "    del df_sub\n",
    "    gc.collect()\n",
    "\n",
    "# Concatenate all the DataFrames\n",
    "df_merged_all = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "# Final info\n",
    "print(\"\\nShape of the final merged DataFrame:\", df_merged_all.shape)\n",
    "\n",
    "# Preview first 5 rows (notebook friendly)\n",
    "df_merged_all.head()\n",
    "# If using a .py script: print(df_merged_all.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d3b74",
   "metadata": {},
   "source": [
    "## Time series analysis of sales\n",
    "\n",
    "### Subtask:\n",
    "Visualize the sales trends over time for selected items or aggregates (e.g., by store, category, or state).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Convert 'date' column to datetime objects\n",
    "df_merged_all['date'] = pd.to_datetime(df_merged_all['date'])\n",
    "\n",
    "# 2Ô∏è‚É£ Aggregate sales data by date\n",
    "daily_sales = df_merged_all.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "# 3Ô∏è‚É£ Create a line plot of total daily sales over time\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.lineplot(data=daily_sales, x='date', y='sales')\n",
    "\n",
    "# 4Ô∏è‚É£ Add a title to the plot\n",
    "plt.title(\"Total Daily Sales Over Time\")\n",
    "\n",
    "# 5Ô∏è‚É£ Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da349c1",
   "metadata": {},
   "source": [
    "## Analyze categorical variables\n",
    "\n",
    "### Subtask:\n",
    "Explore the distributions of categorical variables like `item_id`, `dept_id`, `cat_id`, `store_id`, and `state_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_cols = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "\n",
    "# Print unique values and top 5 frequencies\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(f\"Number of unique values: {df_merged_all[col].nunique()}\")\n",
    "    print(f\"Top 5 most frequent values:\\n{df_merged_all[col].value_counts().head()}\")\n",
    "\n",
    "# Create count plots for selected categorical variables\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 18))\n",
    "\n",
    "sns.countplot(\n",
    "    data=df_merged_all, \n",
    "    y='cat_id', \n",
    "    ax=axes[0], \n",
    "    order=df_merged_all['cat_id'].value_counts().index\n",
    ")\n",
    "axes[0].set_title('Distribution of Categories')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_ylabel('Category ID')\n",
    "\n",
    "sns.countplot(\n",
    "    data=df_merged_all, \n",
    "    y='store_id', \n",
    "    ax=axes[1], \n",
    "    order=df_merged_all['store_id'].value_counts().index\n",
    ")\n",
    "axes[1].set_title('Distribution of Stores')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_ylabel('Store ID')\n",
    "\n",
    "sns.countplot(\n",
    "    data=df_merged_all, \n",
    "    y='state_id', \n",
    "    ax=axes[2], \n",
    "    order=df_merged_all['state_id'].value_counts().index\n",
    ")\n",
    "axes[2].set_title('Distribution of States')\n",
    "axes[2].set_xlabel('Count')\n",
    "axes[2].set_ylabel('State ID')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Separate count plot for departments\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(\n",
    "    data=df_merged_all, \n",
    "    y='dept_id', \n",
    "    order=df_merged_all['dept_id'].value_counts().index\n",
    ")\n",
    "plt.title('Distribution of Departments')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Department ID')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e82fd",
   "metadata": {},
   "source": [
    "## Analyze event impacts\n",
    "\n",
    "### Subtask:\n",
    "Investigate the impact of `event_name_1`, `event_type_1`, `event_name_2`, and `event_type_2` on sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4edc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# List of event columns\n",
    "event_cols = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "\n",
    "# 1Ô∏è‚É£ Add new categories and replace missing values\n",
    "for col in event_cols:\n",
    "    # Ensure the column is categorical\n",
    "    if not pd.api.types.is_categorical_dtype(df_merged_all[col]):\n",
    "        df_merged_all[col] = df_merged_all[col].astype('category')\n",
    "    \n",
    "    new_category = f'no_{col.split(\"_\")[1]}'\n",
    "    if new_category not in df_merged_all[col].cat.categories:\n",
    "        df_merged_all[col] = df_merged_all[col].cat.add_categories(new_category)\n",
    "    \n",
    "    df_merged_all[col] = df_merged_all[col].fillna(new_category)\n",
    "\n",
    "# 2Ô∏è‚É£ Compute average sales per event/event type\n",
    "event_name_1_sales = df_merged_all.groupby('event_name_1')['sales'].mean().sort_values(ascending=False)\n",
    "event_type_1_sales = df_merged_all.groupby('event_type_1')['sales'].mean().sort_values(ascending=False)\n",
    "event_name_2_sales = df_merged_all.groupby('event_name_2')['sales'].mean().sort_values(ascending=False)\n",
    "event_type_2_sales = df_merged_all.groupby('event_type_2')['sales'].mean().sort_values(ascending=False)\n",
    "\n",
    "# 3Ô∏è‚É£ Print results\n",
    "print(\"\\n-- Average Sales by event_name_1 --\")\n",
    "print(event_name_1_sales)\n",
    "print(\"\\n-- Average Sales by event_type_1 --\")\n",
    "print(event_type_1_sales)\n",
    "print(\"\\n-- Average Sales by event_name_2 --\")\n",
    "print(event_name_2_sales)\n",
    "print(\"\\n-- Average Sales by event_type_2 --\")\n",
    "print(event_type_2_sales)\n",
    "\n",
    "# 4Ô∏è‚É£ Visualize average sales per event/event type\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 15))\n",
    "\n",
    "sns.barplot(x=event_name_1_sales.index, y=event_name_1_sales.values, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Average Sales by event_name_1')\n",
    "axes[0, 0].set_xlabel('Event Name 1')\n",
    "axes[0, 0].set_ylabel('Average Sales')\n",
    "axes[0, 0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(x=event_type_1_sales.index, y=event_type_1_sales.values, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Average Sales by event_type_1')\n",
    "axes[0, 1].set_xlabel('Event Type 1')\n",
    "axes[0, 1].set_ylabel('Average Sales')\n",
    "axes[0, 1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(x=event_name_2_sales.index, y=event_name_2_sales.values, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Average Sales by event_name_2')\n",
    "axes[1, 0].set_xlabel('Event Name 2')\n",
    "axes[1, 0].set_ylabel('Average Sales')\n",
    "axes[1, 0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.barplot(x=event_type_2_sales.index, y=event_type_2_sales.values, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Average Sales by event_type_2')\n",
    "axes[1, 1].set_xlabel('Event Type 2')\n",
    "axes[1, 1].set_ylabel('Average Sales')\n",
    "axes[1, 1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc0246",
   "metadata": {},
   "source": [
    "## Analyze snap day impacts\n",
    "\n",
    "### Subtask:\n",
    "Examine the effect of SNAP days (`snap_CA`, `snap_TX`, `snap_WI`) on sales for the respective states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Examine the effect of SNAP days\n",
    "\n",
    "# Average sales by state and SNAP status for CA\n",
    "snap_sales_ca = df_merged_all.groupby(['state_id', 'snap_CA'])['sales'].mean().reset_index()\n",
    "print(\"\\n-- Average Sales by State (CA) and SNAP Status --\")\n",
    "print(snap_sales_ca[snap_sales_ca['state_id'] == 'CA'])\n",
    "\n",
    "# Average sales by state and SNAP status for TX\n",
    "snap_sales_tx = df_merged_all.groupby(['state_id', 'snap_TX'])['sales'].mean().reset_index()\n",
    "print(\"\\n-- Average Sales by State (TX) and SNAP Status --\")\n",
    "print(snap_sales_tx[snap_sales_tx['state_id'] == 'TX'])\n",
    "\n",
    "# 2Ô∏è‚É£ Create bar plots for SNAP vs. non-SNAP sales for each state\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
    "\n",
    "# Plot for CA\n",
    "sns.barplot(\n",
    "    data=snap_sales_ca[snap_sales_ca['state_id'] == 'CA'], \n",
    "    x='snap_CA', \n",
    "    y='sales', \n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Average Sales on SNAP vs. Non-SNAP Days (CA)')\n",
    "axes[0].set_xlabel('SNAP Status')\n",
    "axes[0].set_ylabel('Average Sales')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xticklabels(['Non-SNAP', 'SNAP'])\n",
    "\n",
    "# Plot for TX\n",
    "sns.barplot(\n",
    "    data=snap_sales_tx[snap_sales_tx['state_id'] == 'TX'], \n",
    "    x='snap_TX', \n",
    "    y='sales', \n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('Average Sales on SNAP vs. Non-SNAP Days (TX)')\n",
    "axes[1].set_xlabel('SNAP Status')\n",
    "axes[1].set_ylabel('Average Sales')\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_xticklabels(['Non-SNAP', 'SNAP'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449bd3fd",
   "metadata": {},
   "source": [
    "## Analyze sell price trends\n",
    "\n",
    "### Subtask:\n",
    "Visualize how sell prices change over time for selected items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff6d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Select a few item_ids to visualize price trends\n",
    "selected_items = ['HOBBIES_1_001', 'FOODS_3_090', 'HOUSEHOLD_1_008']\n",
    "\n",
    "# 2Ô∏è‚É£ Filter the df_merged_all DataFrame for selected items\n",
    "df_selected_items = df_merged_all[df_merged_all['item_id'].isin(selected_items)].copy()\n",
    "\n",
    "# Ensure 'date' is in datetime format\n",
    "df_selected_items['date'] = pd.to_datetime(df_selected_items['date'])\n",
    "\n",
    "# 3Ô∏è‚É£ Create a line plot\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.lineplot(data=df_selected_items, x='date', y='sell_price', hue='item_id')\n",
    "\n",
    "# 4Ô∏è‚É£ Add title and labels\n",
    "plt.title(\"Sell Price Trends Over Time for Selected Items\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sell Price\")\n",
    "\n",
    "# 5Ô∏è‚É£ Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8daed",
   "metadata": {},
   "source": [
    "## Analyze relationship between sales and price\n",
    "\n",
    "### Subtask:\n",
    "Explore the relationship between sales and sell price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1Ô∏è‚É£ Scatter plot to visualize relationship between sell_price and sales\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=df_merged_all,\n",
    "    x='sell_price',\n",
    "    y='sales',\n",
    "    alpha=0.1  # makes dense points more visible\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Add title and axis labels\n",
    "plt.title(\"Relationship between Sales and Sell Price\")\n",
    "plt.xlabel(\"Sell Price\")\n",
    "plt.ylabel(\"Sales\")\n",
    "\n",
    "# 3Ô∏è‚É£ Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca123d",
   "metadata": {},
   "source": [
    "## Analyze day of week and month effects\n",
    "\n",
    "> Add blockquote\n",
    "\n",
    "\n",
    "\n",
    "### Subtask:\n",
    "Investigate if there are patterns in sales based on the day of the week or month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43125b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Extract day of week and month from date\n",
    "df_merged_all['day_of_week'] = df_merged_all['date'].dt.day_name()\n",
    "df_merged_all['month'] = df_merged_all['date'].dt.month_name()\n",
    "\n",
    "# 2Ô∏è‚É£ Calculate average sales\n",
    "average_sales_by_weekday = df_merged_all.groupby('day_of_week')['sales'].mean()\n",
    "average_sales_by_month = df_merged_all.groupby('month')['sales'].mean()\n",
    "\n",
    "# 3Ô∏è‚É£ Display results\n",
    "print(\"Average Sales by Day of the Week:\")\n",
    "print(average_sales_by_weekday)\n",
    "\n",
    "print(\"\\nAverage Sales by Month:\")\n",
    "print(average_sales_by_month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a5d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order for days of the week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Create bar plot for average sales by day of the week\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=average_sales_by_weekday.index, y=average_sales_by_weekday.values, order=day_order)\n",
    "plt.title('Average Sales by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.show()\n",
    "\n",
    "# Define the order for months (optional, but good practice)\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Create bar plot for average sales by month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=average_sales_by_month.index, y=average_sales_by_month.values, order=month_order)\n",
    "plt.title('Average Sales by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670087a5",
   "metadata": {},
   "source": [
    "## Analyze sales by store, category, and department\n",
    "\n",
    "### Subtask:\n",
    "Visualize and compare sales across different stores, categories, and departments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af00e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Average sales by store\n",
    "average_sales_by_store = df_merged_all.groupby('store_id')['sales'].mean().reset_index()\n",
    "print(\"Average Sales by Store:\")\n",
    "print(average_sales_by_store)\n",
    "\n",
    "# 2Ô∏è‚É£ Average sales by category\n",
    "average_sales_by_category = df_merged_all.groupby('cat_id')['sales'].mean().reset_index()\n",
    "print(\"\\nAverage Sales by Category:\")\n",
    "print(average_sales_by_category)\n",
    "\n",
    "# 3Ô∏è‚É£ Average sales by department\n",
    "average_sales_by_department = df_merged_all.groupby('dept_id')['sales'].mean().reset_index()\n",
    "print(\"\\nAverage Sales by Department:\")\n",
    "print(average_sales_by_department)\n",
    "\n",
    "# 4Ô∏è‚É£ Bar plot for average sales by store\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='store_id', y='sales', data=average_sales_by_store)\n",
    "plt.title('Average Sales by Store')\n",
    "plt.xlabel('Store ID')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.show()\n",
    "\n",
    "# 5Ô∏è‚É£ Bar plot for average sales by category\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='cat_id', y='sales', data=average_sales_by_category)\n",
    "plt.title('Average Sales by Category')\n",
    "plt.xlabel('Category ID')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.show()\n",
    "\n",
    "# 6Ô∏è‚É£ Bar plot for average sales by department\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='dept_id', y='sales', data=average_sales_by_department)\n",
    "plt.title('Average Sales by Department')\n",
    "plt.xlabel('Department ID')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0141a3d",
   "metadata": {},
   "source": [
    "## Identify outliers and anomalies\n",
    "\n",
    "### Subtask:\n",
    "Look for any unusual patterns or outliers in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b868fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1Ô∏è‚É£ Box plot of 'sales' by store_id\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='store_id', y='sales', data=df_merged_all)\n",
    "plt.title('Sales Distribution by Store ID')\n",
    "plt.xlabel('Store ID')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n",
    "\n",
    "# 2Ô∏è‚É£ Box plot of 'sales' by cat_id\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='cat_id', y='sales', data=df_merged_all)\n",
    "plt.title('Sales Distribution by Category ID')\n",
    "plt.xlabel('Category ID')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n",
    "\n",
    "# 3Ô∏è‚É£ Box plot of 'sales' by dept_id\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='dept_id', y='sales', data=df_merged_all)\n",
    "plt.title('Sales Distribution by Department ID')\n",
    "plt.xlabel('Department ID')\n",
    "plt.ylabel('Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4Ô∏è‚É£ Box plot of 'sell_price' to examine outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(y='sell_price', data=df_merged_all)\n",
    "plt.title('Distribution of Sell Price')\n",
    "plt.ylabel('Sell Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b929ed2",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "\n",
    "*   The dataset contains sales data for two stores (CA\\_1 and TX\\_1) with a total of 11,836,218 rows and 22 columns.\n",
    "*   Total daily sales show a clear increasing trend over time with noticeable seasonality.\n",
    "*   The dataset has a large number of unique items (3049), while other categorical variables like department, category, store, and state have significantly fewer unique values.\n",
    "*   The `FOODS` category and the `FOODS_3` department are the most frequent in the dataset.\n",
    "*   The distribution of data across stores and states is perfectly balanced between the two stores (CA\\_1 and TX\\_1) and two states (CA and TX).\n",
    "*   Certain events, such as OrthodoxEaster, Easter, and LaborDay, are associated with higher average sales, while Christmas, Thanksgiving, and NewYear are associated with lower average sales.\n",
    "*   Sporting and Cultural events tend to have slightly higher average sales compared to Religious and National events.\n",
    "*   SNAP days have a positive impact on average sales in both California and Texas.\n",
    "*   Sell prices for selected items show fluctuations over time.\n",
    "*   A scatter plot of sales versus sell price suggests a general inverse relationship, where higher prices are associated with lower sales, although there is significant variability.\n",
    "*   Sales are significantly higher on weekends (Saturday and Sunday) compared to weekdays.\n",
    "*   Average sales show some variation by month, with August and July having slightly higher averages.\n",
    "*   Average sales vary significantly across stores, categories, and departments, with CA\\_1, FOODS, and FOODS\\_3 showing the highest average sales in their respective groups.\n",
    "*   Box plots indicate the presence of outliers in sales distribution across stores, categories, and departments, as well as in the distribution of sell prices.\n",
    "\n",
    "### Insights or Next Steps\n",
    "\n",
    "*   The identified seasonality and event impacts suggest that these factors should be strongly considered in any sales forecasting model.\n",
    "*   The significant differences in sales performance across stores, categories, and departments indicate that a granular approach to sales analysis and forecasting at these levels would be beneficial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35027547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Load merged data for store CA_1\n",
    "df_ca1 = pd.read_pickle(\"processed/merged_CA_1.pkl\")  # adjust path if saved in 'processed/' folder\n",
    "\n",
    "print(\"Shape of df_ca1:\", df_ca1.shape)\n",
    "\n",
    "# 2Ô∏è‚É£ Display the first 3 rows\n",
    "print(\"\\nFirst 3 rows of df_ca1:\")\n",
    "print(df_ca1.head(3))\n",
    "\n",
    "# 3Ô∏è‚É£ Check available columns\n",
    "print(\"\\nColumns in df_ca1:\")\n",
    "print(df_ca1.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Convert \"d\" (like \"d_1\") into an integer day index\n",
    "df_ca1[\"d_num\"] = df_ca1[\"d\"].str[2:].astype(int)\n",
    "\n",
    "print(\"Added 'd_num' column. First few values:\")\n",
    "print(df_ca1[[\"d\", \"d_num\"]].head(5))\n",
    "\n",
    "# 2Ô∏è‚É£ Take a sample of 100,000 rows for faster exploration\n",
    "df_ca1_sample = df_ca1.sample(100_000, random_state=42)\n",
    "\n",
    "print(\"\\nSampled 100,000 rows. Shape:\", df_ca1_sample.shape)\n",
    "print(df_ca1_sample.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c968370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Group by \"date\" to calculate average sales per day in the sample\n",
    "avg_sales_by_date = (\n",
    "    df_ca1_sample\n",
    "    .groupby(\"date\", as_index=False)[\"sales\"]\n",
    "    .mean()\n",
    "    .rename(columns={\"sales\": \"avg_sales_sample\"})\n",
    ")\n",
    "\n",
    "print(\"Shape of avg_sales_by_date:\", avg_sales_by_date.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(avg_sales_by_date.head(5))\n",
    "\n",
    "# 2Ô∏è‚É£ Optional: Sort by date for chronological view\n",
    "avg_sales_by_date = avg_sales_by_date.sort_values(\"date\")\n",
    "print(\"\\nFirst 5 rows after sorting by date:\")\n",
    "print(avg_sales_by_date.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ae0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1Ô∏è‚É£ Plot average sales over time\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.lineplot(data=avg_sales_by_date, x=\"date\", y=\"avg_sales_sample\")\n",
    "plt.title(\"Daily Average Sales (Sample of 100k rows)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Sales\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Sort by 'id' and 'd_num' to prepare for lag calculation\n",
    "df_ca1_sample = df_ca1_sample.sort_values([\"id\", \"d_num\"]).copy()\n",
    "\n",
    "# 2Ô∏è‚É£ Create a 7-day lag feature for 'sales'\n",
    "df_ca1_sample[\"sales_lag7\"] = (\n",
    "    df_ca1_sample\n",
    "    .groupby(\"id\")[\"sales\"]\n",
    "    .shift(7)\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Inspect the first 10 rows to check the lag feature\n",
    "print(\"Created 'sales_lag7' feature (lag=7 days). First 10 rows:\")\n",
    "print(df_ca1_sample[[\"id\", \"d_num\", \"sales\", \"sales_lag7\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e31aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Missing Lag Values and Splitting for a Simple Train/Validation\n",
    "# 1) Fill missing values in 'sales_lag7' with 0 (a quick choice)\n",
    "df_ca1_sample[\"sales_lag7\"] = df_ca1_sample[\"sales_lag7\"].fillna(0)\n",
    "\n",
    "# 2) Define a cutoff, for example d_num < 1500 as \"train\", d_num >= 1500 as \"val\"\n",
    "train_mask = df_ca1_sample[\"d_num\"] < 1500\n",
    "val_mask   = df_ca1_sample[\"d_num\"] >= 1500\n",
    "\n",
    "df_train = df_ca1_sample[train_mask].copy()\n",
    "df_val   = df_ca1_sample[val_mask].copy()\n",
    "\n",
    "print(\"Training set shape:\", df_train.shape)\n",
    "print(\"Validation set shape:\", df_val.shape)\n",
    "\n",
    "# Show a sample of each\n",
    "print(\"\\nSample rows from training set:\")\n",
    "print(df_train.sample(5))\n",
    "\n",
    "print(\"\\nSample rows from validation set:\")\n",
    "print(df_val.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6386b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Combine train + validation sets for feature engineering (optional)\n",
    "df_all = pd.concat([df_train, df_val], axis=0, sort=False).copy()\n",
    "df_all = df_all.sort_values([\"id\", \"d_num\"]).reset_index(drop=True)\n",
    "\n",
    "# 2Ô∏è‚É£ Define lags and rolling windows\n",
    "lags = [7, 14, 28]\n",
    "rolling_windows = [7, 28]\n",
    "\n",
    "# 3Ô∏è‚É£ Group by item id\n",
    "grouped = df_all.groupby(\"id\", observed=False)\n",
    "\n",
    "# 4Ô∏è‚É£ Create lag features\n",
    "for lag in lags:\n",
    "    df_all[f\"sales_lag{lag}\"] = grouped[\"sales\"].shift(lag)\n",
    "\n",
    "# 5Ô∏è‚É£ Create rolling mean features (shift by 1 to avoid using current day's value)\n",
    "for w in rolling_windows:\n",
    "    df_all[f\"sales_rollmean{w}\"] = grouped[\"sales\"].shift(1).rolling(w, min_periods=1).mean()\n",
    "\n",
    "# 6Ô∏è‚É£ Fill missing values for all new features\n",
    "feature_cols = [f\"sales_lag{x}\" for x in lags] + [f\"sales_rollmean{x}\" for x in rolling_windows]\n",
    "df_all[feature_cols] = df_all[feature_cols].fillna(0)\n",
    "\n",
    "print(\"Created multiple lag/rolling features. Sample of columns:\")\n",
    "print(df_all[[\"id\", \"d_num\", \"sales\"] + feature_cols].head(10))\n",
    "\n",
    "# 7Ô∏è‚É£ Re-split into train and validation sets\n",
    "df_train = df_all[df_all[\"d_num\"] < 1500].copy()\n",
    "df_val   = df_all[df_all[\"d_num\"] >= 1500].copy()\n",
    "\n",
    "print(f\"\\nTrain shape: {df_train.shape}, Validation shape: {df_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ced6f5",
   "metadata": {},
   "source": [
    "## Build and Train LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define our feature columns and target\n",
    "feature_cols = [f\"sales_lag{x}\" for x in [7, 14, 28]] + [f\"sales_rollmean{x}\" for x in [7, 28]]\n",
    "target_col = \"sales\"\n",
    "\n",
    "# Combine train and validation sets to split again into train, test, and validation\n",
    "df_all = pd.concat([df_train, df_val], axis=0, sort=False).copy()\n",
    "df_all = df_all.sort_values([\"id\", \"d_num\"]).reset_index(drop=True)\n",
    "\n",
    "# Define a cutoff for train/test/validation split\n",
    "# Let's use d_num < 1500 for training, 1500 <= d_num < 1700 for testing, and d_num >= 1700 for validation\n",
    "train_mask = df_all[\"d_num\"] < 1500\n",
    "test_mask  = (df_all[\"d_num\"] >= 1500) & (df_all[\"d_num\"] < 1700)\n",
    "val_mask   = df_all[\"d_num\"] >= 1700\n",
    "\n",
    "df_train = df_all[train_mask].copy()\n",
    "df_test  = df_all[test_mask].copy()\n",
    "df_val   = df_all[val_mask].copy()\n",
    "\n",
    "print(f\"Training set shape: {df_train.shape}\")\n",
    "print(f\"Testing set shape: {df_test.shape}\")\n",
    "print(f\"Validation set shape: {df_val.shape}\")\n",
    "\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train[target_col]\n",
    "\n",
    "X_test   = df_test[feature_cols]\n",
    "y_test   = df_test[target_col]\n",
    "\n",
    "X_val   = df_val[feature_cols]\n",
    "y_val   = df_val[target_col]\n",
    "\n",
    "\n",
    "print(\"\\nUsing features:\", feature_cols)\n",
    "\n",
    "# Initialize and fit the LightGBM model on the training data\n",
    "model_lgbm = LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "pred_test = model_lgbm.predict(X_test)\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, pred_test))\n",
    "print(\"\\nTest RMSE:\", rmse_test)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "pred_val_lgbm = model_lgbm.predict(X_val)\n",
    "\n",
    "print(\"\\nPredictions on validation set made.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d122398",
   "metadata": {},
   "source": [
    "## Evaluate LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d154e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate the LightGBM model on the validation set\n",
    "pred_val_lgbm = model_lgbm.predict(X_val)\n",
    "rmse_lgbm = np.sqrt(mean_squared_error(y_val, pred_val_lgbm))\n",
    "print(f\"LightGBM Validation RMSE: {rmse_lgbm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c1a6c",
   "metadata": {},
   "source": [
    "## Build and Train XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training XGBoost with Lag and Rolling Features\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error # Import mean_squared_error\n",
    "\n",
    "# Define our feature columns and target\n",
    "feature_cols = [f\"sales_lag{x}\" for x in [7, 14, 28]] + [f\"sales_rollmean{x}\" for x in [7, 28]]\n",
    "target_col = \"sales\"\n",
    "\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train[target_col]\n",
    "\n",
    "X_val   = df_val[feature_cols]\n",
    "y_val   = df_val[target_col]\n",
    "\n",
    "print(\"Using features:\", feature_cols)\n",
    "\n",
    "# Initialize and fit the model\n",
    "model_xgb = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model_xgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)]\n",
    "    # eval_metric=\"rmse\" # Removed eval_metric as it's not supported in this version\n",
    ")\n",
    "\n",
    "# Predict on validation\n",
    "pred_val_xgb = model_xgb.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_val, pred_val_xgb))\n",
    "print(\"XGBoost Validation RMSE:\", rmse_xgb)\n",
    "\n",
    "# best_iteration_ may not be available if early stopping is not used\n",
    "# print(\"Best iteration:\", model.best_iteration_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf399c",
   "metadata": {},
   "source": [
    "## Evaluate XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation\n",
    "pred_val_xgb = model_xgb.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_val, pred_val_xgb))\n",
    "print(\"XGBoost Validation RMSE:\", rmse_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bcde3",
   "metadata": {},
   "source": [
    "## Build and Train Random Forest Regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training RandomForest with Lag and Rolling Features\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define our feature columns and target\n",
    "feature_cols = [f\"sales_lag{x}\" for x in [7, 14, 28]] + [f\"sales_rollmean{x}\" for x in [7, 28]]\n",
    "target_col = \"sales\"\n",
    "\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train[target_col]\n",
    "\n",
    "X_val   = df_val[feature_cols]\n",
    "y_val   = df_val[target_col]\n",
    "\n",
    "print(\"Using features:\", feature_cols)\n",
    "\n",
    "# Initialize and fit the model\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation\n",
    "pred_val_rf = model_rf.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_val, pred_val_rf))\n",
    "print(\"RandomForest Validation RMSE:\", rmse_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d98829",
   "metadata": {},
   "source": [
    "## Evaluate RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60168c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation\n",
    "pred_val_rf = model_rf.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_val, pred_val_rf))\n",
    "print(\"RandomForest Validation RMSE:\", rmse_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25bdbd",
   "metadata": {},
   "source": [
    "## Build and Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# Reshape the data for LSTM input (samples, time steps, features)\n",
    "X_train_lstm = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "y_train_lstm = y_train.values\n",
    "\n",
    "X_val_lstm = X_val.values.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "y_val_lstm = y_val.values\n",
    "\n",
    "\n",
    "# 1. Define the LSTM model\n",
    "model_lstm = Sequential()\n",
    "\n",
    "# 2. Add an LSTM layer\n",
    "# The input shape is (time steps, number of features)\n",
    "model_lstm.add(LSTM(units=50, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "model_lstm.add(Dropout(0.2)) # Add dropout for regularization\n",
    "\n",
    "# 3. Add a Dense layer for the output\n",
    "model_lstm.add(Dense(units=1))\n",
    "\n",
    "# 4. Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 5. Print the model summary\n",
    "model_lstm.summary()\n",
    "\n",
    "# Train the LSTM model\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    epochs=50, # Use 50 epochs as specified\n",
    "    batch_size=32, # Use 32 as specified batch size\n",
    "    validation_split=0.2 # Use validation split of 0.2\n",
    ")\n",
    "\n",
    "print(\"\\nLSTM model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e8d39c",
   "metadata": {},
   "source": [
    "## Evaluate LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate LSTM model\n",
    "pred_val_lstm = model_lstm.predict(X_val_lstm)\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_val_lstm, pred_val_lstm))\n",
    "print(f\"LSTM Validation RMSE: {rmse_lstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e6a33",
   "metadata": {},
   "source": [
    "## Build and Train Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# Reshape the data for LSTM input (samples, time steps, features)\n",
    "# Using the same reshaping as for the standard LSTM and GRU models\n",
    "X_train_lstm = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "y_train_lstm = y_train.values\n",
    "\n",
    "X_val_lstm = X_val.values.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "y_val_lstm = y_val.values\n",
    "\n",
    "\n",
    "# 1. Define the Bidirectional LSTM model\n",
    "model_bi_lstm = Sequential()\n",
    "\n",
    "# 2. Add a Bidirectional LSTM layer\n",
    "# The input shape is (time steps, number of features)\n",
    "model_bi_lstm.add(Bidirectional(LSTM(units=50, return_sequences=False), input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "model_bi_lstm.add(Dropout(0.2)) # Add dropout for regularization\n",
    "\n",
    "# 3. Add a Dense layer for the output\n",
    "model_bi_lstm.add(Dense(units=1))\n",
    "\n",
    "# 4. Compile the model\n",
    "model_bi_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 5. Print the model summary\n",
    "model_bi_lstm.summary()\n",
    "\n",
    "# Train the Bidirectional LSTM model\n",
    "history_bi_lstm = model_bi_lstm.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    epochs=50, # Use 50 epochs as specified\n",
    "    batch_size=32, # Use 32 as specified batch size\n",
    "    validation_split=0.2 # Use validation split of 0.2\n",
    ")\n",
    "\n",
    "print(\"\\nBidirectional LSTM model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d87296",
   "metadata": {},
   "source": [
    "## Evaluate Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate Bidirectional LSTM model\n",
    "pred_val_bi_lstm = model_bi_lstm.predict(X_val_lstm)\n",
    "rmse_bi_lstm = np.sqrt(mean_squared_error(y_val_lstm, pred_val_bi_lstm))\n",
    "print(f\"Bidirectional LSTM Validation RMSE: {rmse_bi_lstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d1c85",
   "metadata": {},
   "source": [
    "## Build and Train GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout # Ensure all necessary layers are imported\n",
    "import numpy as np\n",
    "\n",
    "# Reshape the data for GRU input (samples, time steps, features) - using the same reshaping as LSTM\n",
    "X_train_gru = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "y_train_gru = y_train.values\n",
    "\n",
    "X_val_gru = X_val.values.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "y_val_gru = y_val.values\n",
    "\n",
    "\n",
    "# 1. Define the GRU model\n",
    "model_gru = Sequential()\n",
    "\n",
    "# 2. Add a GRU layer\n",
    "# The input shape is (time steps, number of features)\n",
    "model_gru.add(GRU(units=50, input_shape=(X_train_gru.shape[1], X_train_gru.shape[2])))\n",
    "model_gru.add(Dropout(0.2)) # Add dropout for regularization\n",
    "\n",
    "# 3. Add a Dense layer for the output\n",
    "model_gru.add(Dense(units=1))\n",
    "\n",
    "# 4. Compile the model\n",
    "model_gru.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 5. Print the model summary\n",
    "model_gru.summary()\n",
    "\n",
    "# Train the GRU model\n",
    "history_gru = model_gru.fit(\n",
    "    X_train_gru, y_train_gru,\n",
    "    epochs=50, # Use 50 epochs as specified\n",
    "    batch_size=32, # Use 32 as specified batch size\n",
    "    validation_split=0.2 # Use validation split of 0.2\n",
    ")\n",
    "\n",
    "print(\"\\nGRU model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b1564",
   "metadata": {},
   "source": [
    "## Evaluate GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3835a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRU model\n",
    "pred_val_gru = model_gru.predict(X_val_gru)\n",
    "rmse_gru = np.sqrt(mean_squared_error(y_val_gru, pred_val_gru))\n",
    "print(f\"GRU Validation RMSE: {rmse_gru}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to hold the RMSE values for each model\n",
    "model_performance = pd.DataFrame({\n",
    "    'Model': ['LightGBM', 'XGBoost', 'RandomForest', 'LSTM', 'GRU', 'Bidirectional LSTM'],\n",
    "    'RMSE': [rmse_lgbm, rmse_xgb, rmse_rf, rmse_lstm, rmse_gru, rmse_bi_lstm]\n",
    "})\n",
    "\n",
    "# Sort by RMSE for better visualization\n",
    "model_performance = model_performance.sort_values(by='RMSE')\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='RMSE', data=model_performance)\n",
    "plt.title('Validation RMSE of Different Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('RMSE')\n",
    "plt.ylim(model_performance['RMSE'].min() * 0.9, model_performance['RMSE'].max() * 1.1) # Adjust y-axis limits for better view\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c554228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained GRU model\n",
    "model_gru.save(\"gru_model.keras\")\n",
    "print(\"GRU model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80732ea",
   "metadata": {},
   "source": [
    "## Dynamic Pricing Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d25c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --- Configuration ---\n",
    "STORE_ID = \"CA_1\"\n",
    "NUM_TOP_ITEMS = 100\n",
    "PRICE_ADJUSTMENT_RANGE_FULL = [-0.15, -0.10, -0.05, 0, 0.05, 0.10, 0.15]\n",
    "PRICE_FLOOR_RATIO = 0.8   # Minimum 80% of base price\n",
    "BASE_ELASTICITY = -1.5\n",
    "MIN_ELASTICITY = -0.2       # For low-demand items\n",
    "SYNTHETIC_PRICE_MULTIPLIER = 1.0  # Use average historical price as base\n",
    "\n",
    "# --- Load Data ---\n",
    "df_ca1_all = pd.read_pickle(f\"merged_{STORE_ID}.pkl\")\n",
    "df_ca1_all[\"d_num\"] = df_ca1_all[\"d\"].str[2:].astype(int)\n",
    "\n",
    "# Load trained GRU model\n",
    "model_gru = load_model(\"gru_model.keras\")\n",
    "\n",
    "# --- Identify Top Items ---\n",
    "sales_eval_store = sales_eval[sales_eval[\"store_id\"] == STORE_ID].copy()\n",
    "date_cols = [c for c in sales_eval_store.columns if c.startswith(\"d_\")]\n",
    "sales_eval_store[\"total_sales\"] = sales_eval_store[date_cols].sum(axis=1)\n",
    "top_100_items = sales_eval_store.sort_values(\"total_sales\", ascending=False).head(NUM_TOP_ITEMS)[[\"id\", \"total_sales\"]]\n",
    "df_ca1_top100_merged = df_ca1_all[df_ca1_all[\"id\"].isin(top_100_items[\"id\"])].copy()\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "df_ca1_top100_merged = df_ca1_top100_merged.sort_values([\"id\", \"d_num\"]).reset_index(drop=True)\n",
    "lags = [7, 14, 28]\n",
    "rolling_windows = [7, 28]\n",
    "grouped_top100 = df_ca1_top100_merged.groupby(\"id\", observed=False)\n",
    "\n",
    "for lag in lags:\n",
    "    df_ca1_top100_merged[f\"sales_lag{lag}\"] = grouped_top100[\"sales\"].shift(lag)\n",
    "for w in rolling_windows:\n",
    "    df_ca1_top100_merged[f\"sales_rollmean{w}\"] = grouped_top100[\"sales\"].shift(1).rolling(w, min_periods=1).mean()\n",
    "\n",
    "feature_cols = [f\"sales_lag{x}\" for x in lags] + [f\"sales_rollmean{x}\" for x in rolling_windows]\n",
    "df_ca1_top100_merged[feature_cols] = df_ca1_top100_merged[feature_cols].fillna(0)\n",
    "\n",
    "# --- Predict Demand using GRU ---\n",
    "X_predict = df_ca1_top100_merged[feature_cols].values.reshape((df_ca1_top100_merged.shape[0], 1, len(feature_cols)))\n",
    "predicted_demand = model_gru.predict(X_predict)\n",
    "df_ca1_top100_merged[\"predicted_demand_raw\"] = predicted_demand.flatten()\n",
    "\n",
    "# --- Scale Predicted Demand to Historical Average ---\n",
    "# Compute historical average sales per item\n",
    "item_avg_sales = df_ca1_top100_merged.groupby(\"id\")[\"sales\"].mean()\n",
    "# Compute predicted average demand per item\n",
    "item_predicted_avg = df_ca1_top100_merged.groupby(\"id\")[\"predicted_demand_raw\"].mean()\n",
    "\n",
    "# Scaling factor = historical / predicted\n",
    "scaling_factor = (item_avg_sales / item_predicted_avg).replace([np.inf, -np.inf, np.nan], 1.0)\n",
    "\n",
    "# Apply scaling\n",
    "df_ca1_top100_merged[\"predicted_demand\"] = df_ca1_top100_merged.apply(\n",
    "    lambda row: max(row[\"predicted_demand_raw\"] * scaling_factor.get(row[\"id\"], 1.0), 0), axis=1\n",
    ")\n",
    "\n",
    "# --- Handle Missing/Zero Sell Prices ---\n",
    "item_avg_prices = df_ca1_top100_merged.groupby(\"id\")[\"sell_price\"].mean()\n",
    "def fill_synthetic_price(row):\n",
    "    if pd.isna(row[\"sell_price\"]) or row[\"sell_price\"] <= 0:\n",
    "        base_price = item_avg_prices.get(row[\"id\"], 1)\n",
    "        return base_price * SYNTHETIC_PRICE_MULTIPLIER\n",
    "    return row[\"sell_price\"]\n",
    "df_ca1_top100_merged[\"sell_price\"] = df_ca1_top100_merged.apply(fill_synthetic_price, axis=1)\n",
    "\n",
    "# --- Dynamic Pricing Optimization ---\n",
    "def calculate_adjusted_demand(current_price, predicted_demand, candidate_price, elasticity):\n",
    "    return predicted_demand * ((candidate_price / current_price) ** elasticity) if current_price > 0 else predicted_demand\n",
    "\n",
    "def calculate_expected_revenue(adjusted_demand, candidate_price):\n",
    "    return max(adjusted_demand * candidate_price, 0)\n",
    "\n",
    "df_optimal_prices = []\n",
    "\n",
    "for index, row in df_ca1_top100_merged.iterrows():\n",
    "    current_price = row[\"sell_price\"]\n",
    "    predicted_demand = row[\"predicted_demand\"]\n",
    "\n",
    "    # Low-demand items get restricted price adjustments and lower elasticity\n",
    "    if predicted_demand < 1:\n",
    "        PRICE_ADJUSTMENT_RANGE = [0, 0.05, 0.10]\n",
    "        elasticity = MIN_ELASTICITY\n",
    "    else:\n",
    "        PRICE_ADJUSTMENT_RANGE = PRICE_ADJUSTMENT_RANGE_FULL\n",
    "        elasticity = BASE_ELASTICITY\n",
    "\n",
    "    best_revenue = -np.inf\n",
    "    optimal_price = current_price\n",
    "\n",
    "    for adjustment in PRICE_ADJUSTMENT_RANGE:\n",
    "        candidate_price = current_price * (1 + adjustment)\n",
    "        candidate_price = max(candidate_price, current_price * PRICE_FLOOR_RATIO)\n",
    "\n",
    "        adjusted_demand = calculate_adjusted_demand(current_price, predicted_demand, candidate_price, elasticity)\n",
    "        adjusted_demand = max(adjusted_demand, 0)\n",
    "        expected_revenue = calculate_expected_revenue(adjusted_demand, candidate_price)\n",
    "\n",
    "        if expected_revenue > best_revenue:\n",
    "            best_revenue = expected_revenue\n",
    "            optimal_price = candidate_price\n",
    "\n",
    "    df_optimal_prices.append({\n",
    "        \"id\": row[\"id\"],\n",
    "        \"date\": row[\"date\"],\n",
    "        \"current_price\": current_price,\n",
    "        \"predicted_demand\": predicted_demand,\n",
    "        \"recommended_price\": optimal_price,\n",
    "        \"expected_revenue\": best_revenue\n",
    "    })\n",
    "\n",
    "df_optimal_prices = pd.DataFrame(df_optimal_prices)\n",
    "\n",
    "# --- Revenue Comparison ---\n",
    "df_eval_elasticity = pd.merge(\n",
    "    df_ca1_top100_merged,\n",
    "    df_optimal_prices,\n",
    "    on=[\"id\", \"date\", \"predicted_demand\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "df_eval_elasticity[\"actual_revenue_historical\"] = df_eval_elasticity[\"sales\"] * df_eval_elasticity[\"sell_price\"]\n",
    "\n",
    "total_actual_revenue_historical = df_eval_elasticity[\"actual_revenue_historical\"].sum()\n",
    "total_estimated_revenue_elasticity = df_eval_elasticity[\"expected_revenue\"].sum()\n",
    "revenue_change_elasticity = total_estimated_revenue_elasticity - total_actual_revenue_historical\n",
    "revenue_change_percentage_elasticity = (revenue_change_elasticity / total_actual_revenue_historical) * 100 if total_actual_revenue_historical > 0 else 0\n",
    "\n",
    "print(f\"Total Actual Revenue: ${total_actual_revenue_historical:,.2f}\")\n",
    "print(f\"Total Estimated Revenue: ${total_estimated_revenue_elasticity:,.2f}\")\n",
    "print(f\"Revenue Change: ${revenue_change_elasticity:,.2f}\")\n",
    "print(f\"Revenue Change %: {revenue_change_percentage_elasticity:.2f}%\")\n",
    "\n",
    "# --- Visualization ---\n",
    "revenue_comparison_eval_data = pd.DataFrame({\n",
    "    'Revenue Type': ['Actual Historical Revenue', 'Estimated Revenue (Dynamic Pricing)'],\n",
    "    'Total Revenue': [total_actual_revenue_historical, total_estimated_revenue_elasticity]\n",
    "})\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='Revenue Type', y='Total Revenue', data=revenue_comparison_eval_data)\n",
    "plt.title(f'Revenue Comparison (Top {NUM_TOP_ITEMS} Items, {STORE_ID})')\n",
    "plt.ylabel('Total Revenue ($)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa554b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --- Configuration ---\n",
    "STORE_ID = \"CA_1\"\n",
    "PRICE_ADJUSTMENT_RANGE_FULL = [-0.15, -0.10, -0.05, 0, 0.05, 0.10, 0.15]\n",
    "PRICE_FLOOR_RATIO = 0.8\n",
    "BASE_ELASTICITY = -1.5\n",
    "MIN_ELASTICITY = -0.2\n",
    "SYNTHETIC_PRICE_MULTIPLIER = 1.0\n",
    "\n",
    "# --- Load Data ---\n",
    "df_ca1_all = pd.read_pickle(f\"merged_{STORE_ID}.pkl\")\n",
    "df_ca1_all[\"d_num\"] = df_ca1_all[\"d\"].str[2:].astype(int)\n",
    "\n",
    "# Load trained GRU model\n",
    "model_gru = load_model(\"gru_model.keras\")\n",
    "\n",
    "# --- Feature Engineering for All Items ---\n",
    "df_ca1_all = df_ca1_all.sort_values([\"id\", \"d_num\"]).reset_index(drop=True)\n",
    "lags = [7, 14, 28]\n",
    "rolling_windows = [7, 28]\n",
    "grouped_all = df_ca1_all.groupby(\"id\", observed=False)\n",
    "\n",
    "for lag in lags:\n",
    "    df_ca1_all[f\"sales_lag{lag}\"] = grouped_all[\"sales\"].shift(lag)\n",
    "for w in rolling_windows:\n",
    "    df_ca1_all[f\"sales_rollmean{w}\"] = grouped_all[\"sales\"].shift(1).rolling(w, min_periods=1).mean()\n",
    "\n",
    "feature_cols = [f\"sales_lag{x}\" for x in lags] + [f\"sales_rollmean{x}\" for x in rolling_windows]\n",
    "df_ca1_all[feature_cols] = df_ca1_all[feature_cols].fillna(0)\n",
    "\n",
    "# --- Predict Demand for All Items ---\n",
    "X_predict = df_ca1_all[feature_cols].values.reshape((df_ca1_all.shape[0], 1, len(feature_cols)))\n",
    "predicted_demand = model_gru.predict(X_predict)\n",
    "df_ca1_all[\"predicted_demand_raw\"] = predicted_demand.flatten()\n",
    "\n",
    "# --- Scale Predicted Demand to Historical Average ---\n",
    "item_avg_sales = df_ca1_all.groupby(\"id\")[\"sales\"].mean()\n",
    "item_predicted_avg = df_ca1_all.groupby(\"id\")[\"predicted_demand_raw\"].mean()\n",
    "scaling_factor = (item_avg_sales / item_predicted_avg).replace([np.inf, -np.inf, np.nan], 1.0)\n",
    "df_ca1_all[\"predicted_demand\"] = df_ca1_all.apply(\n",
    "    lambda row: max(row[\"predicted_demand_raw\"] * scaling_factor.get(row[\"id\"], 1.0), 0), axis=1\n",
    ")\n",
    "\n",
    "# --- Handle Missing/Zero Sell Prices ---\n",
    "item_avg_prices = df_ca1_all.groupby(\"id\")[\"sell_price\"].mean()\n",
    "def fill_synthetic_price(row):\n",
    "    if pd.isna(row[\"sell_price\"]) or row[\"sell_price\"] <= 0:\n",
    "        base_price = item_avg_prices.get(row[\"id\"], 1)\n",
    "        return base_price * SYNTHETIC_PRICE_MULTIPLIER\n",
    "    return row[\"sell_price\"]\n",
    "df_ca1_all[\"sell_price\"] = df_ca1_all.apply(fill_synthetic_price, axis=1)\n",
    "\n",
    "# --- Dynamic Pricing Optimization for All Items ---\n",
    "def calculate_adjusted_demand(current_price, predicted_demand, candidate_price, elasticity):\n",
    "    return predicted_demand * ((candidate_price / current_price) ** elasticity) if current_price > 0 else predicted_demand\n",
    "\n",
    "def calculate_expected_revenue(adjusted_demand, candidate_price):\n",
    "    return max(adjusted_demand * candidate_price, 0)\n",
    "\n",
    "df_optimal_prices = []\n",
    "\n",
    "for index, row in df_ca1_all.iterrows():\n",
    "    current_price = row[\"sell_price\"]\n",
    "    predicted_demand = row[\"predicted_demand\"]\n",
    "\n",
    "    if predicted_demand < 1:\n",
    "        PRICE_ADJUSTMENT_RANGE = [0, 0.05, 0.10]\n",
    "        elasticity = MIN_ELASTICITY\n",
    "    else:\n",
    "        PRICE_ADJUSTMENT_RANGE = PRICE_ADJUSTMENT_RANGE_FULL\n",
    "        elasticity = BASE_ELASTICITY\n",
    "\n",
    "    best_revenue = -np.inf\n",
    "    optimal_price = current_price\n",
    "\n",
    "    for adjustment in PRICE_ADJUSTMENT_RANGE:\n",
    "        candidate_price = current_price * (1 + adjustment)\n",
    "        candidate_price = max(candidate_price, current_price * PRICE_FLOOR_RATIO)\n",
    "\n",
    "        adjusted_demand = calculate_adjusted_demand(current_price, predicted_demand, candidate_price, elasticity)\n",
    "        adjusted_demand = max(adjusted_demand, 0)\n",
    "        expected_revenue = calculate_expected_revenue(adjusted_demand, candidate_price)\n",
    "\n",
    "        if expected_revenue > best_revenue:\n",
    "            best_revenue = expected_revenue\n",
    "            optimal_price = candidate_price\n",
    "\n",
    "    df_optimal_prices.append({\n",
    "        \"id\": row[\"id\"],\n",
    "        \"date\": row[\"date\"],\n",
    "        \"current_price\": current_price,\n",
    "        \"predicted_demand\": predicted_demand,\n",
    "        \"recommended_price\": optimal_price,\n",
    "        \"expected_revenue\": best_revenue\n",
    "    })\n",
    "\n",
    "df_optimal_prices = pd.DataFrame(df_optimal_prices)\n",
    "\n",
    "# --- Revenue Comparison ---\n",
    "df_eval_elasticity = pd.merge(\n",
    "    df_ca1_all,\n",
    "    df_optimal_prices,\n",
    "    on=[\"id\", \"date\", \"predicted_demand\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "df_eval_elasticity[\"actual_revenue_historical\"] = df_eval_elasticity[\"sales\"] * df_eval_elasticity[\"sell_price\"]\n",
    "\n",
    "total_actual_revenue_historical = df_eval_elasticity[\"actual_revenue_historical\"].sum()\n",
    "total_estimated_revenue_elasticity = df_eval_elasticity[\"expected_revenue\"].sum()\n",
    "revenue_change_elasticity = total_estimated_revenue_elasticity - total_actual_revenue_historical\n",
    "revenue_change_percentage_elasticity = (revenue_change_elasticity / total_actual_revenue_historical) * 100 if total_actual_revenue_historical > 0 else 0\n",
    "\n",
    "print(f\"Total Actual Revenue: ${total_actual_revenue_historical:,.2f}\")\n",
    "print(f\"Total Estimated Revenue: ${total_estimated_revenue_elasticity:,.2f}\")\n",
    "print(f\"Revenue Change: ${revenue_change_elasticity:,.2f}\")\n",
    "print(f\"Revenue Change %: {revenue_change_percentage_elasticity:.2f}%\")\n",
    "\n",
    "# --- Visualization ---\n",
    "revenue_comparison_eval_data = pd.DataFrame({\n",
    "    'Revenue Type': ['Actual Historical Revenue', 'Estimated Revenue (Dynamic Pricing)'],\n",
    "    'Total Revenue': [total_actual_revenue_historical, total_estimated_revenue_elasticity]\n",
    "})\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='Revenue Type', y='Total Revenue', data=revenue_comparison_eval_data)\n",
    "plt.title(f'Revenue Comparison (All Items, {STORE_ID})')\n",
    "plt.ylabel('Total Revenue ($)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be0b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "import gc\n",
    "import os # Import os for listing files\n",
    "\n",
    "# --- Configuration ---\n",
    "# Moved inside the function to be store-specific if needed, or keep outside if global\n",
    "NUM_TOP_ITEMS = 1000  # Increased number of top items\n",
    "PRICE_ADJUSTMENT_RANGE_FULL = [-0.15, -0.10, -0.05, 0, 0.05, 0.10, 0.15]\n",
    "PRICE_FLOOR_RATIO = 0.8   # Minimum 80% of base price\n",
    "BASE_ELASTICITY = -1.5\n",
    "MIN_ELASTICITY = -0.2       # For low-demand items\n",
    "SYNTHETIC_PRICE_MULTIPLIER = 1.0  # Use average historical price as base\n",
    "\n",
    "# --- Define the Pricing Pipeline Function ---\n",
    "def run_pricing_pipeline_select_file():\n",
    "    print(\"\\n--- Running Dynamic Pricing Pipeline ---\")\n",
    "\n",
    "    # --- Select Data File ---\n",
    "    try:\n",
    "        print(\"Available merged data files in /content/:\")\n",
    "        # List all .pkl files in the /content/ directory\n",
    "        available_files = [f for f in os.listdir('/content/') if f.endswith('.pkl') and f.startswith('merged_')]\n",
    "        if not available_files:\n",
    "            print(\"No merged data files found in /content/.\")\n",
    "            return\n",
    "\n",
    "        for i, file_name in enumerate(available_files):\n",
    "            print(f\"{i+1}: {file_name}\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                choice = int(input(f\"Please enter the number of the file you want to use (1-{len(available_files)}): \"))\n",
    "                if 1 <= choice <= len(available_files):\n",
    "                    selected_file_name = available_files[choice - 1]\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Invalid choice. Please enter a number within the range.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "        print(f\"Selected file: '{selected_file_name}'\")\n",
    "\n",
    "        # Infer store ID from the filename (assuming format merged_STORE_ID.pkl)\n",
    "        try:\n",
    "            store_id = selected_file_name.split('_')[1].split('.')[0]\n",
    "            print(f\"Inferred Store ID: {store_id}\")\n",
    "        except IndexError:\n",
    "             print(\"Could not infer Store ID from filename. Please ensure the filename is in the format 'merged_STORE_ID.pkl'\")\n",
    "             store_id = \"Unknown_Store\" # Assign a default or handle as error\n",
    "\n",
    "\n",
    "        df_store_all = pd.read_pickle(os.path.join('/content/', selected_file_name))\n",
    "        df_store_all[\"d_num\"] = df_store_all[\"d\"].str[2:].astype(int)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data file selection or loading: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Load trained GRU model\n",
    "    try:\n",
    "        model_gru = load_model(\"gru_model.keras\")\n",
    "        print(\"GRU model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GRU model: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Identify Top Items ---\n",
    "    try:\n",
    "        # Load sales_train_evaluation.csv to get total sales for item ranking\n",
    "        # This assumes sales_train_evaluation.csv is still available in the environment\n",
    "        sales_eval_path = \"/content/sales_train_evaluation.csv\"\n",
    "        if not os.path.exists(sales_eval_path):\n",
    "             print(f\"Error: Required file '{sales_eval_path}' not found. Please ensure it's in the /content directory.\")\n",
    "             return\n",
    "\n",
    "        sales_eval = pd.read_csv(sales_eval_path)\n",
    "        sales_eval_store = sales_eval[sales_eval[\"store_id\"] == store_id].copy()\n",
    "\n",
    "        if sales_eval_store.empty:\n",
    "             print(f\"Warning: No sales data found for store ID '{store_id}' in sales_train_evaluation.csv. Cannot identify top items.\")\n",
    "             # Attempt to proceed with all items if no store-specific sales_eval data is found\n",
    "             print(\"Proceeding with all items in the selected file for this store.\")\n",
    "             df_store_top_merged = df_store_all.copy()\n",
    "             NUM_TOP_ITEMS_ACTUAL = len(df_store_top_merged[\"id\"].unique())\n",
    "             print(f\"Using {NUM_TOP_ITEMS_ACTUAL} items for analysis.\")\n",
    "        else:\n",
    "            date_cols = [c for c in sales_eval_store.columns if c.startswith(\"d_\")]\n",
    "            sales_eval_store[\"total_sales\"] = sales_eval_store[date_cols].sum(axis=1)\n",
    "            top_items = sales_eval_store.sort_values(\"total_sales\", ascending=False).head(NUM_TOP_ITEMS)[\"id\"].tolist()\n",
    "            df_store_top_merged = df_store_all[df_store_all[\"id\"].isin(top_items)].copy()\n",
    "            NUM_TOP_ITEMS_ACTUAL = len(top_items)\n",
    "            print(f\"Identified top {NUM_TOP_ITEMS_ACTUAL} items.\")\n",
    "\n",
    "        del sales_eval, sales_eval_store\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error identifying top items: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # --- Feature Engineering ---\n",
    "    try:\n",
    "        df_store_top_merged = df_store_top_merged.sort_values([\"id\", \"d_num\"]).reset_index(drop=True)\n",
    "        lags = [7, 14, 28]\n",
    "        rolling_windows = [7, 28]\n",
    "        grouped_top = df_store_top_merged.groupby(\"id\", observed=False)\n",
    "\n",
    "        for lag in lags:\n",
    "            df_store_top_merged[f\"sales_lag{lag}\"] = grouped_top[\"sales\"].shift(lag)\n",
    "        for w in rolling_windows:\n",
    "            df_store_top_merged[f\"sales_rollmean{w}\"] = grouped_top[\"sales\"].shift(1).rolling(w, min_periods=1).mean()\n",
    "\n",
    "        feature_cols = [f\"sales_lag{x}\" for x in lags] + [f\"sales_rollmean{x}\" for x in rolling_windows]\n",
    "        df_store_top_merged[feature_cols] = df_store_top_merged[feature_cols].fillna(0)\n",
    "        print(\"Feature engineering complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature engineering: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Predict Demand using GRU ---\n",
    "    try:\n",
    "        # Ensure feature_cols are present in the DataFrame before selecting\n",
    "        missing_features = [col for col in feature_cols if col not in df_store_top_merged.columns]\n",
    "        if missing_features:\n",
    "             print(f\"Error: Missing feature columns after engineering: {missing_features}\")\n",
    "             return\n",
    "\n",
    "        X_predict = df_store_top_merged[feature_cols].values.reshape((df_store_top_merged.shape[0], 1, len(feature_cols)))\n",
    "        predicted_demand = model_gru.predict(X_predict)\n",
    "        df_store_top_merged[\"predicted_demand_raw\"] = predicted_demand.flatten()\n",
    "        print(\"Demand prediction complete.\")\n",
    "\n",
    "        # --- Scale Predicted Demand to Historical Average ---\n",
    "        item_avg_sales = df_store_top_merged.groupby(\"id\")[\"sales\"].mean()\n",
    "        item_predicted_avg = df_store_top_merged.groupby(\"id\")[\"predicted_demand_raw\"].mean()\n",
    "\n",
    "        # Scaling factor = historical / predicted\n",
    "        # Handle cases where item_predicted_avg might be zero or item_avg_sales is zero\n",
    "        scaling_factor = (item_avg_sales / item_predicted_avg)\n",
    "        scaling_factor = scaling_factor.replace([np.inf, -np.inf], np.nan).fillna(1.0) # Replace inf with NaN and fill NaN with 1.0\n",
    "\n",
    "\n",
    "        # Apply scaling\n",
    "        df_store_top_merged[\"predicted_demand\"] = df_store_top_merged.apply(\n",
    "            lambda row: max(row[\"predicted_demand_raw\"] * scaling_factor.get(row[\"id\"], 1.0), 0), axis=1\n",
    "        )\n",
    "        print(\"Predicted demand scaled.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during demand prediction or scaling: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # --- Handle Missing/Zero Sell Prices ---\n",
    "    try:\n",
    "        # Calculate item_avg_prices only for the items in df_store_top_merged\n",
    "        item_avg_prices = df_store_top_merged.groupby(\"id\")[\"sell_price\"].mean()\n",
    "\n",
    "        def fill_synthetic_price(row):\n",
    "            if pd.isna(row[\"sell_price\"]) or row[\"sell_price\"] <= 0:\n",
    "                # Use the average price calculated only for the top items\n",
    "                base_price = item_avg_prices.get(row[\"id\"], 1.0) # Default to 1.0 if item not in calculated averages\n",
    "                return base_price * SYNTHETIC_PRICE_MULTIPLIER\n",
    "            return row[\"sell_price\"]\n",
    "        df_store_top_merged[\"sell_price\"] = df_store_top_merged.apply(fill_synthetic_price, axis=1)\n",
    "        print(\"Missing/zero sell prices handled.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error handling sell prices: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Dynamic Pricing Optimization ---\n",
    "    try:\n",
    "        def calculate_adjusted_demand(current_price, predicted_demand, candidate_price, elasticity):\n",
    "            # Ensure current_price is not zero to avoid division by zero\n",
    "            if current_price <= 0:\n",
    "                 return predicted_demand  # Or handle as appropriate for your case\n",
    "            # Apply elasticity formula\n",
    "            adjusted_demand = predicted_demand * ((candidate_price / current_price) ** elasticity)\n",
    "            return adjusted_demand\n",
    "\n",
    "        def calculate_expected_revenue(adjusted_demand, candidate_price):\n",
    "            return max(adjusted_demand * candidate_price, 0) # Ensure non-negative revenue\n",
    "\n",
    "\n",
    "        df_optimal_prices = []\n",
    "\n",
    "        for index, row in df_store_top_merged.iterrows():\n",
    "            current_price = row[\"sell_price\"]\n",
    "            predicted_demand = row[\"predicted_demand\"]\n",
    "\n",
    "            # Low-demand items get restricted price adjustments and lower elasticity\n",
    "            # Using a more robust check for low predicted demand\n",
    "            if predicted_demand < 0.5: # Threshold can be adjusted\n",
    "                PRICE_ADJUSTMENT_RANGE = [0, 0.05, 0.10] # Offer discounts or no change\n",
    "                elasticity = MIN_ELASTICITY\n",
    "            else:\n",
    "                PRICE_ADJUSTMENT_RANGE = PRICE_ADJUSTMENT_RANGE_FULL # Full range of adjustments\n",
    "                elasticity = BASE_ELASTICITY\n",
    "\n",
    "            best_revenue = -np.inf\n",
    "            optimal_price = current_price\n",
    "\n",
    "            for adjustment in PRICE_ADJUSTMENT_RANGE:\n",
    "                candidate_price = current_price * (1 + adjustment)\n",
    "                candidate_price = max(candidate_price, current_price * PRICE_FLOOR_RATIO) # Apply price floor\n",
    "\n",
    "                adjusted_demand = calculate_adjusted_demand(current_price, predicted_demand, candidate_price, elasticity)\n",
    "                adjusted_demand = max(adjusted_demand, 0) # Ensure non-negative demand\n",
    "                expected_revenue = calculate_expected_revenue(adjusted_demand, candidate_price)\n",
    "\n",
    "                if expected_revenue > best_revenue:\n",
    "                    best_revenue = expected_revenue\n",
    "                    optimal_price = candidate_price\n",
    "\n",
    "            df_optimal_prices.append({\n",
    "                \"id\": row[\"id\"],\n",
    "                \"date\": row[\"date\"],\n",
    "                \"current_price\": current_price,\n",
    "                \"predicted_demand\": predicted_demand,\n",
    "                \"recommended_price\": optimal_price,\n",
    "                \"expected_revenue\": best_revenue\n",
    "            })\n",
    "\n",
    "        df_optimal_prices = pd.DataFrame(df_optimal_prices)\n",
    "        print(\"Dynamic pricing optimization complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during dynamic pricing optimization: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Revenue Comparison ---\n",
    "    try:\n",
    "        # Ensure the merge keys exist in both dataframes\n",
    "        merge_keys = [\"id\", \"date\", \"predicted_demand\"]\n",
    "        if not all(key in df_store_top_merged.columns for key in merge_keys):\n",
    "             print(f\"Error: Missing merge keys in df_store_top_merged: {list(set(merge_keys) - set(df_store_top_merged.columns))}\")\n",
    "             return\n",
    "        if not all(key in df_optimal_prices.columns for key in merge_keys):\n",
    "             print(f\"Error: Missing merge keys in df_optimal_prices: {list(set(merge_keys) - set(df_optimal_prices.columns))}\")\n",
    "             return\n",
    "\n",
    "\n",
    "        df_eval_elasticity = pd.merge(\n",
    "            df_store_top_merged,\n",
    "            df_optimal_prices,\n",
    "            on=merge_keys, # Merge on multiple keys\n",
    "            how=\"left\"\n",
    "        )\n",
    "        df_eval_elasticity[\"actual_revenue_historical\"] = df_eval_elasticity[\"sales\"] * df_eval_elasticity[\"sell_price\"]\n",
    "\n",
    "        total_actual_revenue_historical = df_eval_elasticity[\"actual_revenue_historical\"].sum()\n",
    "        total_estimated_revenue_elasticity = df_eval_elasticity[\"expected_revenue\"].sum()\n",
    "        revenue_change_elasticity = total_estimated_revenue_elasticity - total_actual_revenue_historical\n",
    "        revenue_change_percentage_elasticity = (revenue_change_elasticity / total_actual_revenue_historical) * 100 if total_actual_revenue_historical > 0 else 0\n",
    "\n",
    "        print(f\"\\n--- Revenue Impact for Store: {store_id} ---\")\n",
    "        print(f\"Total Actual Revenue: ${total_actual_revenue_historical:,.2f}\")\n",
    "        print(f\"Total Estimated Revenue: ${total_estimated_revenue_elasticity:,.2f}\")\n",
    "        print(f\"Revenue Change: ${revenue_change_elasticity:,.2f}\")\n",
    "        print(f\"Revenue Change %: {revenue_change_percentage_elasticity:.2f}%\")\n",
    "\n",
    "        # --- Visualization ---\n",
    "        revenue_comparison_data = pd.DataFrame({\n",
    "            'Revenue Type': ['Actual Historical Revenue', 'Estimated Revenue (Dynamic Pricing)']\n",
    "            'Total Revenue': [total_actual_revenue_historical, total_estimated_revenue_elasticity]\n",
    "        })\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(x='Revenue Type', y='Total Revenue', data=revenue_comparison_data)\n",
    "        plt.title(f'Revenue Comparison (Top {NUM_TOP_ITEMS_ACTUAL} Items, {store_id})')\n",
    "        plt.ylabel('Total Revenue ($)')\n",
    "        plt.show()\n",
    "        print(\"Revenue comparison plot generated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during revenue comparison or visualization: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    del df_store_all, df_store_top_merged, df_optimal_prices, df_eval_elasticity\n",
    "    gc.collect()\n",
    "    print(f\"Cleanup complete for store: {store_id}\")\n",
    "\n",
    "# --- Run the Pipeline ---\n",
    "run_pricing_pipeline_select_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
